<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Chris Howard</title>
    <link>https://ckhoward.github.io/blog/index.xml</link>
    <description>Recent content in Blog on Chris Howard</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 27 Dec 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://ckhoward.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>An Introduction to the Stylo Library [R]</title>
      <link>https://ckhoward.github.io/blog/an-introduction-to-the-stylo-library-r/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/an-introduction-to-the-stylo-library-r/</guid>
      <description>

&lt;h1 id=&#34;an-introduction-to-the-stylo-library&#34;&gt;An Introduction to the Stylo Library&lt;/h1&gt;

&lt;h3 id=&#34;what-is-stylometry&#34;&gt;What is Stylometry?&lt;/h3&gt;

&lt;p&gt;Stylometry uses linguistic style to determine who authored some anonymous piece of writing, and it has diverse applications. The authorship of some suicide notes may be questionable. Most forum users have aliases in an attempt to anonymize themselves. And some authors publish their writings under pseudonyms. In these varying cases, stylometry can be used to deanonymize an author.&lt;/p&gt;

&lt;h3 id=&#34;what-is-stylo&#34;&gt;What is Stylo?&lt;/h3&gt;

&lt;p&gt;Stylo is a library for the R programming language that is used for conducting stylometric analyses. Stylometry sounds intimidating, but this library makes these linguistic analyses so simple that its users do not have to be computational linguists. Stylo isn&amp;rsquo;t programmatically intensive and mostly just requires a particular naming convention for files and directories. If users are uncomfortable with writing out functions with varying arguments, they can opt to use a simple GUI.&lt;/p&gt;

&lt;h3 id=&#34;a-stylometric-analysis-of-run-the-jewels&#34;&gt;A Stylometric Analysis of Run the Jewels&lt;/h3&gt;

&lt;p&gt;Run the Jewels is a group of two rappers, El-P and Killer Mike. In collaborative songs, most assume that each rapper raps the verses that they write. This is an interesting example that can be used to show how the Stylo library works for authorship attribution. Models will be trained on lyrics of each rapper when they worked independently, and will then be fed Run the Jewels lyrics to get an idea of each artist&amp;rsquo;s creative footprint in a song, and how collaboration might work. This analysis will serve as a concrete example for some of the uses and quirks of Stylo.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-data&#34;&gt;Getting the data&lt;/h2&gt;

&lt;p&gt;To determine if the project was viable—or that each artist had a distinctive style—I had to get the data. In this case, the data is lyrics that are added to a corpus. Using Enrico Bacis&amp;rsquo; &lt;a href=&#34;https://github.com/enricobacis/lyricwikia&#34;&gt;LyricWikia API&lt;/a&gt; to get lyrics, I use a Bash loop that iterates over an array of hardcoded song names to save the lyrics into the corpus.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/bash elp.png&#34; alt=&#34;Bash Array&#34; title=&#34;The Bash Array&#34; /&gt;
&lt;img src=&#34;img/lyrics elp.png&#34; alt=&#34;Corpus&#34; title=&#34;The Corpus&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Stylo R library is heavily reliant on naming conventions, both for directories and files. Everything before an underscore is treated as a class to be analyzed, so if you are looking at texts of George R.R. Martin, you would do something like GeorgeRRMartin_ASongofIceandFire and the software will understand that George R.R. Martin is an author of interest, responsible for that work. I did the same with El-P and Killer Mike, otherwise each author would not have been treated as unique.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/fixed_formatting.jpg&#34; alt=&#34;New Structure&#34; title=&#34;Changing the naming conventions&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;project-viability-exploratory-analysis&#34;&gt;Project Viability / Exploratory Analysis&lt;/h2&gt;

&lt;p&gt;Now that we have the data, we can do a bit of exploration to check whether the project is viable, in this case, whether each rapper has a distinctive style. This can be achieved with a principal components analysis. Since the file naming convention has been altered for Stylo use, the directory naming still has to be altered, and then the function will work when called.&lt;/p&gt;

&lt;h3 id=&#34;the-stylo-function&#34;&gt;The stylo() function&lt;/h3&gt;

&lt;p&gt;Intuitively enough, one of the most powerful functions in the Stylo library is &lt;code&gt;stylo()&lt;/code&gt;. Running this function will load the GUI and the users can then customize the analysis. The GUI that loads appears as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/gui1.jpg&#34; alt=&#34;Stylo Input&#34; title=&#34;Stylo GUI Input&#34; /&gt;
&lt;img src=&#34;img/gui2.jpg&#34; alt=&#34;Stylo Features&#34; title=&#34;Stylo GUI Features&#34; /&gt;
&lt;img src=&#34;img/gui3.jpg&#34; alt=&#34;Stylo Statistics&#34; title=&#34;Stylo GUI Statistics&#34; /&gt;
&lt;img src=&#34;img/gui4.jpg&#34; alt=&#34;Stylo Sampling&#34; title=&#34;Stylo GUI Sampling&#34; /&gt;
&lt;img src=&#34;img/gui5.jpg&#34; alt=&#34;Stylo Output&#34; title=&#34;Stylo GUI Output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We will go into detail about how the parameters work in an intermediate Stylo post, but for now, only directory information is important. The working directory must be set in R. For this example, it is &lt;code&gt;setwd(&amp;quot;E\\Stylometry\\Run the Jewels&amp;quot;)&lt;/code&gt;. The &lt;code&gt;stylo()&lt;/code&gt; function requires a directory called &lt;code&gt;corpus_files&lt;/code&gt; if it is being run without explicit arguments (i.e. as stylo()). This &lt;code&gt;corpus_files&lt;/code&gt; directory will contain all of the files that you want in your analysis, in this case, all songs by El-P and Killer Mike that I want within the PCA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/corpus_files.jpg&#34; alt=&#34;Stylo corpus_files&#34; title=&#34;Stylo Corpus Files&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After running the function above, with the parameters shown, a PCA is generated. As shown, the library uses the naming convention to color-code each author. El-P is red and Killer Mike is green. It is evident that the rappers differ from each other stylistically when writing music, so the project seems viable. We can proceed to using some of the more interesting tools that Stylo offers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/rtj pca.png&#34; alt=&#34;PCA Visualization&#34; title=&#34;PCA Visualization&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;quick-recap&#34;&gt;Quick Recap&lt;/h3&gt;

&lt;p&gt;All that is needed to generate this PCA is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Proper naming of the directory/corpus&lt;/li&gt;
&lt;li&gt;Proper naming of files in the directory/corpus&lt;/li&gt;
&lt;li&gt;A simple one-line function call&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;using-the-svm-classification-algorithm&#34;&gt;Using the SVM Classification Algorithm&lt;/h2&gt;

&lt;p&gt;Stylo&amp;rsquo;s classification power is what lures most of its users. With other tools, building a classifier and cleaning the data takes several lines of code that aren&amp;rsquo;t too straight-forward, increasing the barrier of entry for those that aren&amp;rsquo;t computationally-inclined. As with the last function, the Stylo library makes this more simple for users, with properly named files and directories, it only takes a single line of code to build and use the classifier.&lt;/p&gt;

&lt;p&gt;There are multiple flavors of classification, but the two functions that we will look at are &lt;code&gt;classify()&lt;/code&gt; and &lt;code&gt;rolling.classify()&lt;/code&gt;. Again, we will get into the technical details in the intermediate post, as this is just a light primer.&lt;/p&gt;

&lt;h3 id=&#34;the-classify-function&#34;&gt;The classify() function&lt;/h3&gt;

&lt;p&gt;Two subdirectories must be present in the working directory for this function to work; &lt;code&gt;primary_set&lt;/code&gt; contains the training data that the classification model will learn from, and &lt;code&gt;secondary_set&lt;/code&gt; will work as the test set. For this Run the Jewels example, the primary_set directory contains 80% (44) of the songs in the total corpus (55), and the secondary_set will contain 20% (11) of the songs. Naturally, each rapper is equally sampled for both directories in order to avoid biasing the model.&lt;/p&gt;

&lt;p&gt;After running &lt;code&gt;classification &amp;lt;- classify()&lt;/code&gt;, a GUI will load allowing me to choose the function&amp;rsquo;s arguments, just like before. Assigning the function to a variable lets us access different values. As shown, from &lt;code&gt;classification$success.rate&lt;/code&gt;, the value of accurately-classified songs is returned, which is 81.8%.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/svm_accuracy.png&#34; alt=&#34;SVM Accuracy&#34; title=&#34;SVM Classification Accuracy&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The SVM model learned El-P&amp;rsquo;s and Killer Mike&amp;rsquo;s linguistic styles from the songs in the primary_set and applied that learning to the songs in the secondary_set, correctly classifying each of those 11 songs. This accuracy is mostly due to the strength of the samples, each song&amp;rsquo;s word count is enough to make the sample powerful. If the files in the directory were smaller, like individual verses, the accuracy is reduced.&lt;/p&gt;

&lt;h3 id=&#34;the-rolling-classify-function&#34;&gt;The rolling.classify() function&lt;/h3&gt;

&lt;p&gt;Two subdirectories must also be present in the working directory for this function to work; &lt;code&gt;reference_set&lt;/code&gt;, which will contain all of the samples that you want the model to learn from, and &lt;code&gt;test_set&lt;/code&gt;, which will contain the questionable or anonymous piece of writing that is of interest. Running the following code generates a visualization of classified samples, color-coded based on the naming convention used.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rolling.info &amp;lt;- rolling.classify(write.png.file = TRUE, classification.method = &amp;quot;svm&amp;quot;, mfw = 300, training.set.sampling = &amp;quot;normal.sampling&amp;quot;,
slice.size = 50, slice.overlap = 45,
milestone.label=c(&amp;quot;El-P&amp;quot;, &amp;quot;Killer Mike&amp;quot;, &amp;quot;El-P&amp;quot;, &amp;quot;Killer Mike&amp;quot;, &amp;quot;Zach de la Rocha&amp;quot;, &amp;quot;Killer Mike&amp;quot;))

title(main=&amp;quot;Run the Jewels \n A Report to the Shareholders / Kill Your Masters&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/RTJ_KYM.png&#34; alt=&#34;RTJ_KYM&#34; title=&#34;Rolling SVM Run the Jewels&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This song, &lt;em&gt;A Report to the Shareholders / Kill Your Masters&lt;/em&gt;, has about 1,000 words in it, as shown by the x-axis. The first 180 words or so are classified as El-P because of the red coloring. This function also has handy labeling functionality, where excerpts can be given vertical labels. For example, if I go into test_set and find the song file that I am interested in, &lt;em&gt;Kill Your Masters&lt;/em&gt;, and add the text &lt;code&gt;xmilestone&lt;/code&gt; before each of the verses, the argument &lt;code&gt;milestone.label&lt;/code&gt; assigns each value to each xmilestone. In the above song, there are six &lt;code&gt;xmilestone&lt;/code&gt; entries, and the function&amp;rsquo;s argument has six corresponding labels. This is useful for comparing classification to some area of interest; in this case, I know which verse belongs to which rapper, so I can manually insert those vertical labels to see if the classifications are &amp;ldquo;accurate.&amp;rdquo; The depth of the colored blocks indicate certainty. Where there are full blocks, there is the most certainty, but where there are dips, there is uncertainty.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;We can see that some of Killer Mike&amp;rsquo;s lyrics are classified as being authored by El-P, and that Zach de la Rocha&amp;rsquo;s verses are also mostly classified as El-P (which makes sense, given the model did not learn de la Rocha&amp;rsquo;s style). The depth of the colored blocks indicate certainty, so it appears that most uncertainty is around verse transition, which could indicate that the two rappers tend to collaborate in this area, perhaps creating a bridge from one rapper&amp;rsquo;s verses, to the other&amp;rsquo;s, so that the song remains fluid. In general, it seems that the model could be biased towards El-P based on the fact that Stylo largely uses MFW (most frequent words).&lt;/p&gt;

&lt;p&gt;With a little tweaking of the filenames and directory names, a single function can be used to clean and classify the data. One line of code takes the place of the several lines that would be used for breaking the text down into the desired number of n-grams, the removal of pronouns, the proper format of text, the building of the model, the training and testing of the data, and even the visualizations. If interested in some of the technical details of how the library works, stay posted for the intermediate-level Stylo post.&lt;/p&gt;

&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;

&lt;p&gt;Stylo (R Library)
Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: a package for computational text analysis. &amp;ldquo;R Journal&amp;rdquo;, 8(1): 107-121.&lt;/p&gt;

&lt;p&gt;Rolling Stylometry
Maciej Eder; Rolling stylometry, Digital Scholarship in the Humanities, Volume 31, Issue 3, 1 September 2016, Pages 457–469, &lt;a href=&#34;https://doi.org/10.1093/llc/fqv010&#34;&gt;https://doi.org/10.1093/llc/fqv010&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;LyricWikia API (Python)
&lt;a href=&#34;https://github.com/enricobacis/lyricwikia&#34;&gt;https://github.com/enricobacis/lyricwikia&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VMs and Usability Testing</title>
      <link>https://ckhoward.github.io/blog/vms-and-usability-testing/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/vms-and-usability-testing/</guid>
      <description>

&lt;h1 id=&#34;vms-for-usability-testing&#34;&gt;VMs for Usability Testing&lt;/h1&gt;

&lt;h2 id=&#34;dependency-nightmares&#34;&gt;Dependency Nightmares&lt;/h2&gt;

&lt;p&gt;This might be an unpopular opinion, but I imagine virtual machines could be a useful tool for usability testers. When it comes to learning a new software or using a new tool, it&amp;rsquo;s the absolute worst finding out that certain dependencies aren&amp;rsquo;t listed in the requirements, or that they are not consistent with your own. This was a pain I experienced when I first started learning Ruby on Rails and MongoDB, and the tutorials I found wouldn&amp;rsquo;t work. The experience was so painful, in fact, that as a young teen, I gave up on these tools. A way to avoid this problem when we create our own tutorials and products may be to use a VM or Container.&lt;/p&gt;

&lt;p&gt;VMs and Containers are a great way to beta-test whatever the team&amp;rsquo;s technical writers produced in terms of &lt;strong&gt;Getting Started&lt;/strong&gt; text, particularly for install steps. Many associate these tools with rigorous scientific work, but fundamentally, they force you to start using some piece of software from scratch.&lt;/p&gt;

&lt;h2 id=&#34;virtualizing-for-a-blank-canvas&#34;&gt;Virtualizing for a Blank Canvas&lt;/h2&gt;

&lt;p&gt;When we start building, our mind becomes a bit warped and our perspective changes as the product changes, and often this perspective is very different from that of a new user. For example, when working on a data pipeline for iNaturalist butterfly data to be processed through a species distribution model, our team went into the project with IPython, Miniconda, Jupyter, Ubuntu, and R all pre-installed on our work machines. When creating the tutorial, we knew Python 3.6, Bash, and R (and its dependencies) were all requirements, so they were listed under the install steps. Then, for the sake of reproducibility, we wanted to build a VM to make it easier for our clients to get their work done.&lt;/p&gt;

&lt;p&gt;We built the VM. Following the tutorial, we installed the requirements, and ran the application. No output. Not only did we realize that R (and its packages) had to be installed through Ubuntu, but we also found some fatal, though easily fixed, directory code. The code worked fine in the builds on our side, but it’s only because somehow, I hadn’t noticed that I created a couple of directories for testing that I hadn’t remembered, but referenced in code. My mind just shifted with the product. Lesson learned, using the VM was a great way of conducting usability testing and testing the soundness of our documentation and code.&lt;/p&gt;

&lt;p&gt;When building products for other people, we want them to have the best experience possible. It is trivially simple to throw together a Virtual Machine or Container to start where a new user may be starting. As nice as it is to be contacted by somebody using your product, it’s not so nice when they express frustration that it isn’t working, or that the tutorial is flawed because it was perfectly followed to no avail. Put yourself in their shoes and do some usability testing with a VM.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Epistemology of Testimony</title>
      <link>https://ckhoward.github.io/blog/the-epistemology-of-testimony/</link>
      <pubDate>Wed, 04 Oct 2017 17:01:31 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/the-epistemology-of-testimony/</guid>
      <description>

&lt;h1 id=&#34;the-epistemology-of-testimony&#34;&gt;The Epistemology of Testimony&lt;/h1&gt;

&lt;p&gt;As humans, our primary way of learning about the world is through experience. But given that we aren&amp;rsquo;t omnipresent and can&amp;rsquo;t experience everything for ourselves, we have to fall back on another method—teamwork. We create information and share it with others, and they do the same. Pretty much everything we learn about the world, apart from what we learn through observation and experience, is based on testimony, or what we hear and read. Naturally, the testimony of others is incredibly important to who we are and what we know.&lt;/p&gt;

&lt;p&gt;Assessing information to be true or false calls for a degree of consciousness. Hume makes an interesting point when he says:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We entertain a suspicion concerning any matter of fact, when the witnesses contradict each other; when they are but few, or of a doubtful character; when they have an interest in what they affirm; when they
deliver their testimony with hesitation, or on the contrary, with too violent asseverations. There are many other particulars of the same kind, which may diminish or destroy the force of any argument, derived from
human testimony.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Our judgments and perceptions shape how we think and feel about who somebody is and if we believe them; what that person is saying; how that person is saying it; and why. Maybe most academics treat this as a subjective probability, on a scale from 0 to 1. I think [-1,1] is a more useful interval, depicted as a spectrum, when thinking about how we interact with the testimony of others. Naturally, for some new event, somebody starts at 0. Depending on our judgments, we may be pushed towards disbelief (-1), or belief (1).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/gradient.jpg&#34; alt=&#34;img&#34; title=&#34;A scale of [-1, 1]&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To get at Hume&amp;rsquo;s points, we should talk about consciousness. Self-awareness is a central issue when it comes to the epistemology of testimony, because the way we deal with other people and information is complex, and often out of focus. More time should be given to understanding how we judge people and the information they convey to us, and how they convey it.&lt;/p&gt;

&lt;h2 id=&#34;judging-communication&#34;&gt;Judging Communication&lt;/h2&gt;

&lt;p&gt;As noted, communicating violently can destroy a person&amp;rsquo;s argument. It doesn&amp;rsquo;t matter if they are right or wrong, an expert or not, once the arguer (or conveyor) of information becomes aggressive, or condescending, or anything with a nasty connotation, the point is lost. Communication really is a key part of knowledge acquisition. It is unfortunate that people are notoriously bad communicators. We become overly defensive at times, have trouble letting others know how we actually feel, and we have a hard time understanding what others actually mean, even if they aren&amp;rsquo;t quite delivering their message as they should be.&lt;/p&gt;

&lt;h2 id=&#34;judging-character&#34;&gt;Judging Character&lt;/h2&gt;

&lt;p&gt;We have to determine a person&amp;rsquo;s authority or credibility so that we can attach some proportion of belief or disbelief to what they tell us. Sometimes language patterns can help us with this; something I have noticed is that conspiracy theorists often have distinctive styles of writing. When I see somebody intermittently throwing in a &amp;lsquo;TRUTH!!!,&amp;rsquo; or calling people &amp;lsquo;sheeple,&amp;rsquo; I automatically attach some (likely great) proportion of disbelief to whatever comes next. When looking at an email that links you to some sign-in form, but you see broken and fragmented Englesh, you may suspect that the sender is not credible, and that it isn&amp;rsquo;t safe to sign in. When it comes to judging other people&amp;rsquo;s anecdotes, I have found one of my personal biases. When I am taking in information, when a source admits it was wrong, but provides &amp;lsquo;evidence&amp;rsquo; immediately afterwards (for what brought his or her revelation), I&amp;rsquo;m much more likely to believe it, or at least my guard is let down. The revelatory aspect is appealing to me, it seems. I probably think an introspective quality in somebody, or that sort of intellectual flexibility to change viewpoint, is something that warrants belief or faith. Reflecting back on Judging Communication, emotion and demeanor can also be ways to judge character.&lt;/p&gt;

&lt;h2 id=&#34;judging-information&#34;&gt;Judging Information&lt;/h2&gt;

&lt;p&gt;Some years back, I stumbled upon a blog post called &lt;a href=&#34;https://youarenotsosmart.com/2011/06/10/the-backfire-effect/&#34;&gt;The Backfire Effect&lt;/a&gt;. It may not be the most scientific source, but I highly recommend reading it because A) it is supported by some sociological studies (for what that is worth) and B) the idea is relevant to communication, and extends to every Facebook argument you have ever seen. The gist of it is that, when presented with information contrary to our beliefs, we have an almost primal inclination to keep our beliefs safe from it. We naturally spend time confirming ideas, finding pieces here and there that are supportive of the idea, and ignoring pieces that are not. And when it comes down to it, this new, contradictory information can actually push us further into believing whatever it is the antagonist tries to counter. When we talk about knowledge acquisition and how it relates to what we are hearing or reading from somebody else, this is the sort of bias that can do a lot of damage in acquiring knowledge. I have seen a phrase pop up that goes &amp;ldquo;strong opinions, loosely held,&amp;rdquo; but I have a difficult time wrapping my head around it. If anyone has any thoughts about this phrase, feel free to share.&lt;/p&gt;

&lt;p&gt;When judging information, timeliness is also a relevant factor. We may see a high-reputation journalist hastily publish details incorrectly. After the break of a big story, where people are still scrambling to gather the facts, it can be useful take pause before consuming and propagating information, as it can have dangerous effects. When considering this Las Vegas shooting, the Gateway Pundit (a political site that some consider credible) &lt;a href=&#34;https://www.nytimes.com/2017/10/02/us/politics/viral-claims-and-rumors-in-the-las-vegas-shooting.html&#34;&gt;wrongly published somebody else being the gunman&lt;/a&gt;, resulting in many death threats being made to the victim and his family. And if view-count makes a YouTube video more credible, just look at this Tweet: &lt;a href=&#34;https://twitter.com/tqbf/status/915630300381745152&#34;&gt;https://twitter.com/tqbf/status/915630300381745152&lt;/a&gt; Given a little bit of time, people will delete their Tweets, edit their published article, or flag and delete videos like the one above.&lt;/p&gt;

&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;d love to know what all of you think about the epistemology of testimony. How do you approach particular comments; which ones seem wise, or ill-informed? What kinds of factors influence the believability of a piece of information for you? Do you have any policies that you use to protect yourself and others from mis- and dis-information? What kinds of responsibilities might be relevant to using testimony as grounds of gaining and sharing knowledge? Do you have any biases in particular that you&amp;rsquo;re interested in? Thanks for reading.&lt;/p&gt;

&lt;h3 id=&#34;sources&#34;&gt;Sources&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://ccl.northwestern.edu/papers/ABMVisualizationGuidelines/palette/scheme-color-scale-gradient.png&#34;&gt;http://ccl.northwestern.edu/papers/ABMVisualizationGuidelines/palette/scheme-color-scale-gradient.png&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Path Distance Analysis [GIS]</title>
      <link>https://ckhoward.github.io/blog/path-distance-analysis-gis/</link>
      <pubDate>Tue, 15 Aug 2017 19:25:23 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/path-distance-analysis-gis/</guid>
      <description>

&lt;h1 id=&#34;path-distance-analysis&#34;&gt;Path-Distance Analysis&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;//This is a fictitious scenario//&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s suppose we&amp;rsquo;re a team of highly skilled NGA agents. We just received a briefing about some suspicious activity between two rival factions. These factions are typically hostile towards each other, but a trusted informant has brought forward some intel suggesting the two factions have been cooperating.&lt;/p&gt;

&lt;p&gt;Our team must assist in verifying these claims. Wielding the all-powerful eye in the sky — or, an array of US Government satellites — we will first identify which outposts are linked to each faction, and we will then infer the inter- and intra-faction transportation networks. This will prove valuable to our client, giving them direction when choosing what to surveil so that they can confirm or deny the informant&amp;rsquo;s claims — that these two factions are indeed working together.&lt;/p&gt;

&lt;p&gt;The two factions are indicated by stars and all of the outposts are indicated with green points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd1.png&#34; alt=&#34;alt text&#34; title=&#34;Aerial shot of two factions within the city&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So we have this overview. By itself, it isn&amp;rsquo;t all that helpful. We need more data if we want to find out which factions own which outposts. Naturally, being the National Geospatial-Intelligence Agency, our team has some useful road-quality data associated with the area.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd2.png&#34; alt=&#34;alt text&#34; title=&#34;digital elevation model for area of interest&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd3.png&#34; alt=&#34;alt text&#34; title=&#34;slopes for area of interest&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;methods-in-path-distance-analysis&#34;&gt;Methods in Path-Distance Analysis&lt;/h2&gt;

&lt;p&gt;The objective is to find out which green points are associated with which stars. You can think of each star as a sort of command post. If we think about it, it&amp;rsquo;s most likely a matter of proximity. You don&amp;rsquo;t want your outposts in enemy territory. So we want to look at path-distance. But it isn&amp;rsquo;t so black and white.&lt;/p&gt;

&lt;h3 id=&#34;euclid-s-distance&#34;&gt;Euclid&amp;rsquo;s Distance&lt;/h3&gt;

&lt;p&gt;Euclidean distance is basically straight-line distance. Below, you can suppose that each ring is approximately a mile long.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd4.png&#34; alt=&#34;alt text&#34; title=&#34;Euclid&#39;s Linear Distance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And if this method were right, we could just find where the rings intersect, divide the area into two, and observe where the outposts fall. If the outposts are in the half containing the red faction, the red faction owns them. If the outposts are in the half containing the blue faction, the blue faction owns them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd5.png&#34; alt=&#34;alt text&#34; title=&#34;Euclid&#39;s Method in Determining Outpost-Faction Relationship&#34; /&gt;
&lt;img src=&#34;img/pd6.png&#34; alt=&#34;alt text&#34; title=&#34;Area being Split by Euclidean Terms&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, paths are more complex. Terrain, wind, road condition, stop-lights, and other elements can influence time to get from point A to point B; you simply can&amp;rsquo;t walk through a mountain. Lucky for us, there are other means of calculating the true distance of paths.&lt;/p&gt;

&lt;h3 id=&#34;arcgis-path-distance&#34;&gt;ArcGIS Path Distance&lt;/h3&gt;

&lt;p&gt;ArcGIS doesn&amp;rsquo;t suffer the pitfalls of the Euclidean method when calculating distance; it comes equipped with a very versatile Path Distance function. As noted, certain environmental factors can slow you down or speed you up on a given path. The ArcGIS function can account for these factors.&lt;/p&gt;

&lt;p&gt;The Path Distance function requires a cost surface, so we will build one. This cost surface is a raster that shows the level of effort required to traverse some area. For simplicity, we will use road quality to account for cost, but GIS software can do a ton more, mathematically incorporating a huge variety of other factors. Intuitively, red areas indicate awful, destroyed roads.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd7.png&#34; alt=&#34;alt text&#34; title=&#34;Road Quality in Cost Surface&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can then &lt;a href=&#34;http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/creating-a-cost-surface-raster.htm&#34;&gt;reclassify&lt;/a&gt; these data. Values are reclassified to more accurately portray cost. In this case, values rise exponentially to reflect how much more effort is used to traverse the different roads. Using a road under full construction isn&amp;rsquo;t possible. Reclassifying data accounts for the relativity of the situation at-hand, as driving a car down a bumpy road will have a different cost than walking down the bumpy road.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd23.png&#34; alt=&#34;alt text&#34; title=&#34;Reclassifying Cost Surface&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd8.png&#34; alt=&#34;alt text&#34; title=&#34;Reclassified Cost Surface&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;path-distance-and-backlink-rasters&#34;&gt;Path Distance and Backlink Rasters&lt;/h2&gt;

&lt;p&gt;Now we can use our Path Distance function. It uses our command posts (the stars) as input points and it uses our cost raster and surface raster to calculate the cost of navigating the area. It will create two new rasters. One is the path distance model and the other is the backlink raster.&lt;/p&gt;

&lt;h3 id=&#34;path-distance-raster&#34;&gt;Path Distance Raster&lt;/h3&gt;

&lt;p&gt;Every cell in this raster holds the least cost accumulated. To put this more simply, it says that we&amp;rsquo;re starting at a command post (either, it doesn&amp;rsquo;t matter which). But we want to leave, without having defined any place we&amp;rsquo;re going. So we start walking. Each step we take, we will have a number of directions we can go, some with higher cost than others (because of road-quality changes). If we think about this in terms of cells, we&amp;rsquo;re on some given cell, and the cells around it have associated cost values. This new raster shows the least cost path whichever way you want to go.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd9.png&#34; alt=&#34;alt text&#34; title=&#34;Area being Split by Euclidean Terms&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;backlink-raster&#34;&gt;Backlink Raster&lt;/h3&gt;

&lt;p&gt;The backlink raster is basically a way-finder, from any (and every) cell, back to the origin (our command posts). The graphic may be a complete mess, seeming entirely incoherent. It&amp;rsquo;s actually really cool though. Each color is associated with a number, and each number is associated with a direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd26.png&#34; alt=&#34;alt text&#34; title=&#34;Back Link Raster Directions from ArcGIS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The number 7 tells you to go north, which in our case is the color yellow. And you follow its directions because it is the least-cost path. In other words, your cell is yellow and telling you to go north because the other cells (NE, E, SE, S, SW, W, NW) all come at a higher cost. Think of it as a color-coded way of getting out of danger.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd21.png&#34; alt=&#34;alt text&#34; title=&#34;Back Link Raster&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The backlink raster will be used to calculate the true relationship between the two command posts and the dozens of outposts. The Cost Path tool takes the smaller outposts as starting points, then uses the instructions of the backlink to make their way back to the command posts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd20.png&#34; alt=&#34;alt text&#34; title=&#34;Cost Path with Backlink&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;Or to make it a little more clear:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd11.png&#34; alt=&#34;alt text&#34; title=&#34;Cost Path without Backlink&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Awesome. So we can finally see the relationships. In this case, it just so happens that Euclid&amp;rsquo;s method was pretty close, it just missed almost all of the points that were immediately on the other side of the division line. Though it took a little extra calculation, factoring in other variables like road-quality turned out to be worth it.&lt;/p&gt;

&lt;h2 id=&#34;inferring-transportation-routes-with-corridor-analysis&#34;&gt;Inferring Transportation Routes with Corridor Analysis&lt;/h2&gt;

&lt;p&gt;Corridor analysis finds the optimal corridor between two points. This is essentially finding the optimal areas of transportation, as opposed to one explicit path. The Corridor tool requires two path distance rasters. Each raster will have least cost paths surrounding some source, in our case, a command post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd27.png&#34; alt=&#34;alt text&#34; title=&#34;Path Distance for Each Source&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The two rasters will then be summed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd14.png&#34; alt=&#34;alt text&#34; title=&#34;Summed Path Distances&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This corridor is hardly insightful. There is too much area between the two factions where transportation is low-cost. We can create thresholds to check for smaller values, so that we can narrow down the number of potential routes. This is done with conditional statements. We&amp;rsquo;re trying to highlight lower-value areas and according to the key, the range of data is [7,690-168,354]. These data vary between projects so trial-and-error is really the best bet for finding a threshold we like. Using Raster Calculator, we will first try finding data that is less than 10,000 with our conditional statement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Con(&amp;ldquo;corridor_surface&amp;rdquo;&amp;lt;10000,1)&lt;/strong&gt; — For each cell, if cell value is less than 10,000, write 1 to cell in output raster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd15.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Threshold 10000&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Con(&amp;ldquo;corridor_surface&amp;rdquo;&amp;lt;9000,1)&lt;/strong&gt; — For each cell, if cell value is less than 9,000, write 1 to cell in output raster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd16.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Threshold 9000&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Pretty good, but we can narrow it down just a little more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Con(&amp;ldquo;corridor_surface&amp;rdquo;&amp;lt;8400,1)&lt;/strong&gt; — For each cell, if cell value is less than 8,400, write 1 to cell in output raster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd17.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Threshold 8400&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is good enough. If we narrow it down too far, we would be unreasonably assuming that these factions use the most optimal route possible to travel between command posts. This isn&amp;rsquo;t realistic. Fortunately, this threshold served its purpose in narrowing down possible routes enough to be insightful. There are three major bottlenecks that could be strategically used to switch out vehicles when tailing a target.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd29.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Overlaid on City&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd30.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Overlaid on City&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With our GIS, we have now inferred the relationship between the two factions and the marked outposts. We also created a great deal of actionable intelligence regarding factions&amp;rsquo; transportation networks. From this, we can consider which areas might be at-risk to violence; which outposts are weaker due to having a low degree of centrality within its faction&amp;rsquo;s network; which outposts are likely to be hotspots; and which areas a surveillence team might work most effectively in.&lt;/p&gt;

&lt;h3 id=&#34;data-source&#34;&gt;Data Source:&lt;/h3&gt;

&lt;p&gt;The University of Arizona&lt;/p&gt;

&lt;p&gt;Advanced GIS (GIST 420)&lt;/p&gt;

&lt;p&gt;Professor Gary Christopherson&lt;/p&gt;

&lt;p&gt;The data used comes from class exercises not related to this scenario.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Analysis of First GOP Debate in 2015 [R]</title>
      <link>https://ckhoward.github.io/blog/sentiment-analysis-of-first-gop-debate-in-2015-r/</link>
      <pubDate>Sat, 17 Jun 2017 14:50:42 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/sentiment-analysis-of-first-gop-debate-in-2015-r/</guid>
      <description>

&lt;h1 id=&#34;a-statistical-analysis-of-sentiments&#34;&gt;A Statistical Analysis of Sentiments&lt;/h1&gt;

&lt;p&gt;Every four years, the United States goes through the process of electing (or re-electing) its president. Politics becomes a popular topic of conversation, and inadvertently, a popular emotional outlet. Our digital landscape has — mostly textually, but sometimes by video or podcast — granted the ability for people to express their thoughts and feelings on political ideas and events, en masse. Suffice to say, an examination of people&amp;rsquo;s language in these expressions can yield many useful insights into human (or &lt;em&gt;American&lt;/em&gt;) character and the influence of rhetoric on political philosophy and national pride. Therefore, to touch on this examination of language, I will be exploring the sentiments (and their respective confidences), of Tweets pertaining to the first GOP debate, that was hosted on August 6th, 2015.&lt;/p&gt;

&lt;h2 id=&#34;the-twitter-sentiment-dataset&#34;&gt;The Twitter-Sentiment Dataset&lt;/h2&gt;

&lt;p&gt;The dataset being explored links Tweets to relevant data such as: issues (e.g. abortion), candidates being responded to (and respective confidence), retweet count, and other metadata, but most importantly, sentiments and their respective confidences. We can load our data into an R dataframe with the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sentiment &amp;lt;- read.csv(&amp;quot;~path/Sentiment.csv&amp;quot;)
View(Sentiment)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot8.png&#34; alt=&#34;The Sentiment Table&#34; title=&#34;The Sentiment.csv Table&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The column labeled &lt;em&gt;sentiment&lt;/em&gt; is what we&amp;rsquo;re interested in so we create two frames, one for negative sentiments, and one for positive. We aren&amp;rsquo;t worried about neutral.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Indexes the sentiment confidence for values associated with positive sentiments
(pos &amp;lt;- Sentiment$sentiment_confidence[Sentiment$sentiment == &amp;quot;Positive&amp;quot;])

#Indexes the sentiment confidence for values associated with negative sentiments
(neg &amp;lt;- Sentiment$sentiment_confidence[Sentiment$sentiment == &amp;quot;Negative&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;confidence-hypotheses&#34;&gt;Confidence Hypotheses&lt;/h2&gt;

&lt;p&gt;My alternative hypothesis is that the mean confidence of negative sentiments will be greater than the mean confidence of positive sentiments; thus, my null hypothesis is that the mean confidence of negative sentiments will be lower than, or equal to, the mean confidence of positive sentiments. The alternative hypothesis is one-tailed.&lt;/p&gt;

&lt;h2 id=&#34;descriptive-statistics-of-sentiment-confidences&#34;&gt;Descriptive Statistics of Sentiment Confidences&lt;/h2&gt;

&lt;p&gt;For this exploration, the mean is chosen as the measure of central tendency because the data is numerical and does not contain outliers. We can find and plot the means of the negative and positive frames quickly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Creates a variable for the mean of the indexed positive sentiment confidence levels
(mean_pos = mean(pos))
#[1] 0.7144841

#Creates a variable for the mean of the indexed negative sentiment confidence levels
(mean_neg = mean(neg))
#[1] 0.8003269

#Creates barplot with a y-scale of [0,1]
barplot(c(mean_pos, mean_neg), names.arg = c(&amp;quot;Positive Sentiments&amp;quot;, &amp;quot;Negative Sentiments&amp;quot;), xlab = &amp;quot;Mean Confidences of Positive and Negative Sentiments&amp;quot;, width = .5, xlim = c(0, 2), ylim = c(0, 1), space = .6, col = c(&amp;quot;darkolivegreen3&amp;quot;, &amp;quot;darkorchid3&amp;quot;), main = &amp;quot;Mean Confidence per Sentiment&amp;quot;, ylab = &amp;quot;Mean Confidence Level&amp;quot;)

#Creates a dotplot with diamond symbol for better visibility
dotchart(c(mean_neg, mean_pos), labels = c(&amp;quot;Negative Sentiments&amp;quot;, &amp;quot;Positive Sentiments&amp;quot;), main = &amp;quot;Mean Confidence per Sentiment&amp;quot;, xlab = &amp;quot;Mean Confidence Level&amp;quot;, color = c(&amp;quot;darkorchid3&amp;quot;, &amp;quot;darkolivegreen3&amp;quot;), pch = 9)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot1.png&#34; alt=&#34;Mean Barplots&#34; title=&#34;Barplot of Positive and Negative Mean Confidences&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/plot2.png&#34; alt=&#34;Mean Dotplots&#34; title=&#34;Dotplot of Positive and Negative Mean Confidences&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Above, there are two representations comparing the mean confidence levels, per positive and negative sentiment, via the sample provided in the dataset, containing 13,871 observations. The first figure shows the different levels of confidence within the scope of the entire scale (0 to 1). It is evident that the mean confidence for negative sentiments is greater than the mean confidence of positive.&lt;/p&gt;

&lt;p&gt;The second figure compares the levels of confidence within the approximate scope of the values themselves (~.70 to ~.81). When calculated, the positive mean confidence is 0.714484 while the negative mean confidence is .800327. Relative to the entire population this is not a large number of observations. Therefore, for a better representation of that population (all Tweets regarding the debate), I will bootstrap resample the observation 10,000 times.&lt;/p&gt;

&lt;h2 id=&#34;inferential-statistics&#34;&gt;Inferential Statistics&lt;/h2&gt;

&lt;p&gt;Bootstrapping will also be done with R code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Resamples pos vector, takes the mean, then replicates 10,000 times for 10,000 means, stores in variable
(resamples_pos = replicate(10000, mean(sample(pos, replace = T))))

#Resamples neg vector, takes the mean, then replicates 10,000 times for 10,000 means, stores in variable
(resamples_neg = replicate(10000, mean(sample(neg, replace = T))))

#Results in distribution of the difference in negative mean distribution and positive mean distribution, stores in variable
(difference_distro = resamples_neg - resamples_pos) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After bootstrap resampling the data 10,000 times, the calculated means are: Mean(positive) = 0.714436 and Mean(negative) = .800329, both staying relatively the same, with Mean(negative - positive) = .0859. Further, the difference in these distributions can be visualized.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Expands margins to fit the entirety of the legend
par(xpd=NA)

#Creates histogram of positive and negative mean distributions for easy comparison
#The add=T argument allows us to add multiple graphs in one plot
positive_distro = hist(resamples_pos, freq = T, col = &amp;quot;darkolivegreen3&amp;quot;, xlim = c(.69, .815), ylim = c(0, 1800), main = &amp;quot;Bootstrapped Mean Confidences per Sentiment (10,000 Resamples per)&amp;quot;, xlab = &amp;quot;Confidence Level&amp;quot;)

#As noted above, add = T adds this histogram distribution to the above histogram
negative_distro = hist(resamples_neg, freq = T, col = &amp;quot;darkorchid3&amp;quot;, add = T)

#Creates a legend for the histogram(s) above; inset contributes to position
legend(&amp;quot;topright&amp;quot;, inset=c(-0.02,0), c(&amp;quot;Positive&amp;quot;, &amp;quot;Negative&amp;quot;), col = c(&amp;quot;darkolivegreen3&amp;quot;, &amp;quot;darkorchid3&amp;quot;), lwd = 5)

#Produces vertical lines for the negative sentiment confidence mean, then for positive
abline(v = mean(resamples_neg), lwd = 2)
abline(v = mean(resamples_pos), lwd = 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot6.png&#34; alt=&#34;Bootstrap Resampling&#34; title=&#34;Bootstrapped Mean Confidences per Sentiment (10,000 Resamples&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Above, we can see the distributions of resampled means in confidence for both positive and negative sentiments, with the black vertical lines indicating the null hypothesis parameter values. This clearly shows that the mean confidence level is greater for the negative sentiment than it is for the positive. This is useful to visualize, but it might be nice to calculate and plot the actual difference in the distributions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Produces distribution of difference between resampled confidences per sentiment
hist(difference_distro, col = &amp;quot;darkslategray3&amp;quot;, freq = F, main = &amp;quot;Difference of Mean Confidence Levels between Sentiments&amp;quot;, xlab = &amp;quot;Confidence Level&amp;quot;, ylab = &amp;quot;Density&amp;quot;)

#Defines the C.I. being used for the hypothesis test
#This is also used for the difference distribution
(CI = quantile(difference_distro, c(0.05, 1))) 
#        5%       100% 
#0.07684648 0.10512917

#Vertical line that shows the C.I. frame 
#This is used to determine if null is rejected or not
abline(v = CI, lwd = 2)

#Further verification that neg - pos is not neg or 0, so null is rejected
(mean_diff = mean(resamples_neg) - mean(resamples_pos))
#[1] 0.08579504
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot7.png&#34; alt=&#34;Difference of Mean Confidences&#34; title=&#34;Difference of Mean Confidence Levels between Sentiments&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This last figure shows the distributed difference between the mean confidences in negative and positive sentiments. As the hypothesis is being tested with a 95% confidence interval, vertical markers have been placed at the 5% mark (at the value of .0769) and at the 100% mark (at the value of .1103).&lt;/p&gt;

&lt;h1 id=&#34;discussion&#34;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;When assessing the null (neg. mean sent. &amp;lt;= pos. mean sent.), we can conclude that the means are not equal as 0 does not lie within the C.I. frame, and we can conclude that the negative mean confidence is not less than the positive, as there are no negative numbers within the C.I. frame. Therefore, the null hypothesis is rejected and the alternative hypothesis is supported. We can say with 95% confidence that the mean confidence of negative sentiments is greater than the mean confidence of positive sentiments. In other words, in 95 of 100 samples, the difference between the mean confidence of negative tweets and the mean confidence of positive tweets will fall within the given interval.&lt;/p&gt;

&lt;p&gt;This might suggest that Twitter could be used, at least in a political context, to vent negative emotions, whether pertinent to politics or not. It could suggest something about these Twitter users in general; perhaps they are mostly liberals, who are more likely to disagree with GOP ideology and be more active online (in social media sites in particular, especially with younger people tending to be more liberal than conservative). Maybe this suggests something about the implicit and explicit nature of positive and negative language, that it&amp;rsquo;s easier to use and identify negative words than it is to use and identify positive words. Maybe candidates tend to appeal to strong emotions (typically negative) in order to appeal to potential voters. Regardless, there are more variables in this dataset that should be considered. Perhaps responses to Trump, or abortions, greatly skew the sentiments. It may be worth breaking the data down further to account for these variables (presidential candidates and issues). To come to a more significant conclusion, it is necessary to delve further into this data, data like it (other GOP debates, democratic debates, and independent debates), and data relevant to these other considerations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; A potential issue with this analysis is the algorithm that was used to assess whether a sentiment is neutral, negative, or positive; human intuition can do a much better job in accounting for context, sarcasm, and other subtleties of human language than language-processing algorithms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Source:&lt;/strong&gt; &lt;a href=&#34;https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment&#34;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Little Bit of Monica in My Life</title>
      <link>https://ckhoward.github.io/blog/a-little-bit-of-monica-in-my-life/</link>
      <pubDate>Tue, 13 Jun 2017 17:01:31 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/a-little-bit-of-monica-in-my-life/</guid>
      <description>

&lt;h1 id=&#34;monica-the-personal-relationship-manager&#34;&gt;Monica, The Personal Relationship Manager&lt;/h1&gt;

&lt;p&gt;For those new to &lt;a href=&#34;https://monicahq.com/&#34;&gt;Monica&lt;/a&gt;, it&amp;rsquo;s a website that allows you to manage your personal relationships. You create contacts and provide information to make sure your encounters are meaningful and frequent (if that&amp;rsquo;s something you care about). You can add information about the contact&amp;rsquo;s significant other, their kids, where they might work, gifts, debts, social media accounts, birthdays, reminders, and even diets. In our busy minds, it can be hard to keep track of so many relevant details, and being human, we sometimes fall short of having &lt;em&gt;great&lt;/em&gt; personal interactions with others. A friend&amp;rsquo;s birthday might be forgotten, we might not check in with our mom as frequently as we&amp;rsquo;d like, and our conversations might not indicate that we truly care about what is going on in a friend&amp;rsquo;s life if we&amp;rsquo;ve forgotten to follow up on the latest major events in their life. These mishaps might not be overtly bad, but they can prevent us from having truly connected relationships. Who wouldn&amp;rsquo;t want better relationships?&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s better, is that Monica open-sources its code and emphasizes privacy. When it&amp;rsquo;s suggested that you aren&amp;rsquo;t being tracked and that your data isn&amp;rsquo;t being sold — a significant promise in an age where everybody is out for your data — it is verifiable.&lt;/p&gt;

&lt;p&gt;I was initially pulled toward the platform, but two pieces of anecdata really spoke to me and cemented my feelings enough to try it out:&lt;/p&gt;

&lt;p&gt;1) &lt;em&gt;danielvf&lt;/em&gt; from Hacker News &lt;a href=&#34;https://news.ycombinator.com/item?id=14498590&#34;&gt;writes&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My uncle died suddenly this year. He was unbelievably caring - and not just to family -
but to everyone he ever met. His funeral was jam packed with everyone
from homeless people to executives of multi-billion dollar companies.&lt;/p&gt;

&lt;p&gt;I always thought that his ability to always have you, and whatever you had
last talked about with him, on his mind at any moment was some kind of
supernatural gift. I was surprised to find out at his funeral that he
actually kept an excel spreadsheet of everyone he met and what they
needed and were going through. He reviewed this constantly.&lt;/p&gt;

&lt;p&gt;It didn&amp;rsquo;t lessen his genuine love for everyone, just let him be a little more super human.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2) In the book &lt;strong&gt;&lt;a href=&#34;https://www.amazon.com/How-Win-Friends-Influence-People/dp/0671027034&#34;&gt;How to Win Friends and Influence People&lt;/a&gt;&lt;/strong&gt;, a tip for getting the most out of the book is suggested:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The president of an important Wall Street bank once described, in
a talk before one of my classes, a highly efficient system he used for
self-improvement. This man had little formal schooling; yet he had
become one of the most important financiers in America, and he
confessed that he owed most of his success to the constant
application of his homemade system. This is what he does, I&amp;rsquo;ll put it
in his own words as accurately as I can remember.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;For years I have kept an engagement book showing all the
appointments I had during the day. My family never made any plans
for me on Saturday night, for the family knew that I devoted a part
of each Saturday evening to the illuminating process of self-examination
and review and appraisal. After dinner I went off by
myself, opened my engagement book, and thought over all the
interviews, discussions and meetings that had taken place during the
week. I asked myself:&lt;/p&gt;

&lt;p&gt;&amp;lsquo;What mistakes did I make that time?&amp;rsquo; &amp;lsquo;What did I do that was right and
in what way could I have improved my performance?&amp;rsquo; &amp;lsquo;What
lessons can I learn from that experience?&amp;rsquo;
&amp;ldquo;I often found that this weekly review made me very unhappy. I was
frequently astonished at my own blunders. Of course, as the years
passed, these blunders became less frequent. Sometimes I was
inclined to pat myself on the back a little after one of these sessions.
This system of self-analysis, self-education, continued year after
year, did more for me than any other one thing I have ever
attempted.
&amp;ldquo;It helped me improve my ability to make decisions - and it aided me
enormously in all my contacts with people. I cannot recommend it
too highly.&amp;rdquo;
Why not use a similar system to check up on your application of the
principles discussed in this book? If you do, two things will result.
First, you will find yourself engaged in an educational process that is
both intriguing and priceless.
Second, you will find that your ability to meet and deal with people
will grow enormously.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think I could use Monica as a tool for holding myself accountable when engaging with others, in a consistent, meaningful way. Monica could also facilitate the sort of introspection noted above, with its journal, activity, and reminder features. But while this is hugely appealing, I have some concerns. What if there was a breach?&lt;/p&gt;

&lt;h1 id=&#34;monica-in-the-crosshairs&#34;&gt;Monica in the Crosshairs&lt;/h1&gt;

&lt;p&gt;A breach would be very dangerous. Arguably Monica stores more information (not data) than Facebook. Where a picture on Facebook might represent an event, its participants, and its location, this is described in plaintext on Monica, making it easier to draw insights. A computer can parse this text much more easily than images. Additionally, since Monica is a privacy-oriented social manager, there is more of a &lt;em&gt;feeling&lt;/em&gt; of personal security, which could likely result in far more personal disclosure, or in other words, the willful release of more intimate details. In some sense, this makes users more vulnerable, personally, if there ever is such a breach. For those that want to draw parallels to encrypted messaging apps, I think the average person is going to write the same content in an SMS as they will a Signal message, and even if not, as of right now these encrypted-messaging apps are far more secure, so the targeting is less of a concern. I will, however, draw parallels to Facebook, because even though Monica isn&amp;rsquo;t a social network, it is in many ways comparable, and can serve many of Facebook&amp;rsquo;s primary functions.&lt;/p&gt;

&lt;p&gt;Notes of private conversations by definition aren&amp;rsquo;t shared on Facebook, but could be on Monica. Details of a relative or friend in a hospital could be implicitly and explicitly exploited. Drug addiction? Financial or spousal trouble? These are all negatives that could be taken advantage of, or could form the basis of a spear-phishing campaign. Even general data like a user&amp;rsquo;s workplace is often faked on Facebook, but on a platform like Monica, people will probably be more inclined to submit honest data.&lt;/p&gt;

&lt;p&gt;Many people first joined Facebook because they wanted to keep track of friends and family in the way Monica does. Facebook largely shifted, became less trustworthy, and became more superficial in users&amp;rsquo; eyes. A social manager that is open-sourced and private (where people are actually themselves and don&amp;rsquo;t self-censor), is an awesome alternative to something like Facebook. Unfortunately, the platform still has an http version that is usable, and the platform still lacks 2FA. These issues will likely be remedied soon, as security is the developer&amp;rsquo;s priority (along with data exporting). Less-savvy users will have to wait for these remedies. Technically proficient users, however, have more options due to Monica being open-source. The growing community has already developed installers for the platform (on Heroku and Docker); users can have much more power over their data by installing Monica on a server they manage. Though this is a good start, it is still worth exploring these considerations before committing large amounts of personal information to a web platform that may be extra prone to being targeted. I&amp;rsquo;m sure more is to come as the platform scales, and I would still easily choose Monica over other platforms any day. I just think users should be at least a little conscious of what kind of data they&amp;rsquo;re adding to their page, without assuming it will be perfectly secure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Projecting with Python [GIS, Python]</title>
      <link>https://ckhoward.github.io/blog/projecting-with-python-gis-python/</link>
      <pubDate>Fri, 09 Jun 2017 12:28:59 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/projecting-with-python-gis-python/</guid>
      <description>

&lt;h1 id=&#34;my-introduction-to-gis-with-python&#34;&gt;My Introduction to GIS with Python&lt;/h1&gt;

&lt;p&gt;Python is a powerful tool in the GIS world, so I wanted to get a little bit of practice with it. I have had a lot of fun working with the &lt;a href=&#34;http://www.start.umd.edu/gtd/&#34;&gt;Global Terrorism Database&lt;/a&gt; so I figured I would go from its CSV format to one that is better-supported by GIS — the shapefile. The dataset contains information related to terrorist attacks, including attack locations. Each location has a variety of data but I will focus on country, latitude, and longitude. Specifically, I will observe attacks in Iraq. The coordinates are based on WGS1984 standards, so they will have to be converted to UTM Zone 38N in order to be mapped on a flat-surface (more on this later, no worries if you don&amp;rsquo;t understand).&lt;/p&gt;

&lt;h1 id=&#34;first-step-cleaning-the-data&#34;&gt;First Step: Cleaning the Data&lt;/h1&gt;

&lt;p&gt;When we first get the data, it contains way more information than we need. There are over 120 variables and as noted, we primarily want to focus on locational data and a few other variables like target type, attack type, and terrorist group responsible. We need to get rid of excess so that our script runs faster and is more relevant to our objective.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/dataset.jpg&#34; alt=&#34;alt text&#34; title=&#34;Global Terrorism Database&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can see the list of imported modules. Pandas is used first to load the data into a DataFrame (basically a table) to be operated on. Functional programming is best suited for the overall task, as for most data analysis, so here is a simple function to clean the data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection1.jpg&#34; alt=&#34;alt text&#34; title=&#34;Cleaning the Data with Pandas Read&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the event that the &lt;code&gt;clean_csv()&lt;/code&gt; hasn&amp;rsquo;t been called yet, call it. If it has already been called, don&amp;rsquo;t worry about it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection2.jpg&#34; alt=&#34;alt text&#34; title=&#34;Cleaning the Data with Pandas Read&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now we&amp;rsquo;ve cleaned our data and created a DataFrame with most of what we want. Now we can use another function to specify which country we&amp;rsquo;re interested in.&lt;/p&gt;

&lt;h1 id=&#34;second-step-specifying-the-country-of-interest&#34;&gt;Second Step: Specifying the Country of Interest&lt;/h1&gt;

&lt;p&gt;In our case, we&amp;rsquo;re interested in looking at Iraq. We want to see all of the terrorist attacks that occurred in Iraq. We will clean the frame a little bit more now. This step could have been done in the first function but I believe it&amp;rsquo;s more simple for a future user to read, understand, and plug the country of interest with a separate function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection3.jpg&#34; alt=&#34;alt text&#34; title=&#34;Specifying the Country of Interest - Iraq&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Simple enough. The DataFrame object from the last function is loaded in and we create a new frame that meets our criteria; where every record&amp;rsquo;s country value is &amp;ldquo;Iraq.&amp;rdquo; Then, any records containing NaNs are dropped. Our coordinate data is still based on the WGS1984 standard — they work on 3D planes, like in Google Earth, but not on 2D planes, like almost every other map in existence. This calls for conversion.&lt;/p&gt;

&lt;h1 id=&#34;third-step-converting-the-coordinates&#34;&gt;Third Step: Converting the Coordinates&lt;/h1&gt;

&lt;p&gt;Coordinate systems use values that assume 3D-space and are provided by satellites. The problem is, most maps are flat, and when you&amp;rsquo;re flattening the earth there are always distortions. These distortions affect distance, direction, and scale. Check out Kai Chang&amp;rsquo;s &lt;a href=&#34;https://bl.ocks.org/syntagmatic/ba569633d51ebec6ec6e&#34;&gt;awesome visualizations&lt;/a&gt; for comparing different map projections.&lt;/p&gt;

&lt;p&gt;Mathematical formulae are used to convert geographic coordinates into their flat-plane equivalents. The Python module &lt;a href=&#34;https://github.com/jswhit/pyproj&#34;&gt;Pyproj&lt;/a&gt; handles this for users, so I import this module and use it in my conversion function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection4.jpg&#34; alt=&#34;alt text&#34; title=&#34;Python Pyproj to Convert from Geographic Coordinates to Projected UTM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You may have noticed I used the UTM projection (Universal Transverse Mercator). This is because distortion occurs at the poles of the earth, whereas the areas around the equator tend to be accurately preserved. This is good for us since we&amp;rsquo;re observing Iraq. Iraq falls in the Zone 38N band, and must be used as an argument.&lt;/p&gt;

&lt;h3 id=&#34;incorrect-pyproj-value-output&#34;&gt;Incorrect Pyproj Value Output&lt;/h3&gt;

&lt;p&gt;Here, I made a common mistake. I was getting pretty wildly wrong values, where points were showing up in Ethiopia and not Iraq. Note the values for utm_lat and utm_lon toward the bottom:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection_output1.jpg&#34; alt=&#34;alt text&#34; title=&#34;Incorrect Pyproj Output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I wanted to verify whether or not these values were right, because I knew upon plotting them, I was getting Ethiopian areas, so I used the &lt;a href=&#34;http://home.hiwaay.net/~taylorc/toolbox/geography/geoutm.html&#34;&gt;Geographic/UTM Coordinate Converter Website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/conversion_site.jpg&#34; alt=&#34;alt text&#34; title=&#34;Coordinate to UTM Web Tool&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Upon looking at the UTM outputs, this is obviously wrong:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;-139638 != 354377&lt;/li&gt;
&lt;li&gt;483126 != 410805&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My problem was that I had the parameters switched. In simple terms, I had p2(latitude, longitude), but I should have had p2(longitude, latitude). It turns out, in many Python GIS modules, longitude comes before latitude. When I changed the order of these parameters, the output was correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection_output2.jpg&#34; alt=&#34;alt text&#34; title=&#34;Correct Pyproj Output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now that all of the outputs are correct, we can go to the final step.&lt;/p&gt;

&lt;h1 id=&#34;fourth-and-final-step-producing-a-shapefile-with-our-new-data&#34;&gt;Fourth and Final Step: Producing a Shapefile with our New Data&lt;/h1&gt;

&lt;p&gt;To create a shapefile I decided I would use Python&amp;rsquo;s Fiona and Shapely modules.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection5.jpg&#34; alt=&#34;alt text&#34; title=&#34;Using Fiona to Create a Shapefile&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The previous two functions are called:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection6.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Convert and Write Functions are Called&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now the shapefile has been created:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection7.jpg&#34; alt=&#34;alt text&#34; title=&#34;Shapefiles in Folder&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;using-arcgis-to-map-the-shapefile&#34;&gt;Using ArcGIS to Map the Shapefile&lt;/h1&gt;

&lt;p&gt;Open ArcMap. Go to the Catalog tab and click it. Navigate to the folder. Find the shapefile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection8.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Shapefile in ArcGIS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Right click the shapefile. Go to properties. In the &amp;lsquo;XY Coordinate System&amp;rsquo; tab, expand the &amp;lsquo;Projected Coordinate Systems&amp;rsquo; folder, expand UTM, expand WGS 1984, expand Northern Hemisphere, and then find &amp;lsquo;WGS 1984 UTM Zone 38N.&amp;rsquo; Click it and press the &amp;lsquo;Apply&amp;rsquo; and &amp;lsquo;OK&amp;rsquo; buttons. Drag the shapefile onto your canvas. And don&amp;rsquo;t forget to go to &amp;lsquo;Add Data&amp;rsquo; and &amp;lsquo;Add Basemap&amp;rsquo; to find a map to sit under the points — I chose the dark gray basemap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection10.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Shapefile Mapped&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As the code indicated, I was also recording information on the target types of the attacks. When I use the identify tool on a point (or attack location), or when I open the attribute table, this information will be associated with the geometry (in our case, the point).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection11.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Shapefile&#39;s Associated Properties&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;These functions are simple but they have vast implications. All I did was take a CSV with spatial data, filtered out data that I wasn&amp;rsquo;t interested in, transformed the data into something that I could actually work with, and then wrote the data to a shapefile. These processes give me the ability to do further geospatial analysis and to build and deploy an interactive and informative website regarding terrorist attacks. In the future, I definitely intend to take advantage of other Python geospatial libraries for map-making, like Descartes and Basemap.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visibility Analysis [GIS]</title>
      <link>https://ckhoward.github.io/blog/visibility-analysis-gis/</link>
      <pubDate>Fri, 26 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/visibility-analysis-gis/</guid>
      <description>

&lt;p&gt;&lt;em&gt;This post is intended to be informative, and not so much reproducible.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;viewshed-analysis&#34;&gt;Viewshed Analysis&lt;/h2&gt;

&lt;p&gt;A viewshed is an area that is visible from an observer at a given location. The problem of visibility lends itself to many domains:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a guard on a prison tower must see the entire prison yard to ensure everyone&amp;rsquo;s safety&lt;/li&gt;
&lt;li&gt;a scout must maximize visibility of a battlefield for intelligence gathering&lt;/li&gt;
&lt;li&gt;autonomous vehicles must have 360-degree visibility&lt;/li&gt;
&lt;li&gt;cell-towers must provide maximum coverage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we will pay mind to the ancient world, where according to Christopherson and Guertin*, &amp;ldquo;fortified sites were often located in order to visually control their territory, sacred sites might be located to provide views of other sacred sites, and the settlement patterns of hinterland sites might be located to facilitate, or to impede, visual communication.&amp;rdquo; In particular, we will be observing and analyzing the Jordanian Umayri Wall&amp;rsquo;s Viewshed. If somebody (that is, say, 6&amp;rsquo; tall), were standing on top of this wall, what would she see?&lt;/p&gt;

&lt;h2 id=&#34;umayri-wall-viewshed&#34;&gt;Umayri Wall Viewshed&lt;/h2&gt;

&lt;p&gt;This is the Viewshed of the Tall Umayri archaeological site. The red star indicates the site, while the white space indicates area that is not visible from the site, and the blue space indicates area that is visible from the site.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri.jpg&#34; alt=&#34;alt text&#34; title=&#34;A Woman&#39;s Visibility when Standing on Wall&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If we want to specifically see the wall, we can zoom in a little bit; the wall is indicated by the red ring. Visualizing what is visible from a given point is a powerful ability. If you noticed, this image is quite pixelated; Viewshed analyses operate on digital elevation models, which are rasters. These analyses will only work on raster data, and not vector data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri2.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Umayri Wall, Indicated by the Red Ring&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;raster-calculation&#34;&gt;Raster Calculation&lt;/h2&gt;

&lt;p&gt;Viewsheds are rasters. GIS allows us to perform raster calculations. In ArcGIS, we can open the raster calculator and use a conditional statement (as shown below) to create a new binary raster; 0 if the cell is not &amp;lsquo;visible&amp;rsquo; and falls within the boundary, and 1 if the cell is visible and within the boundary.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri4.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Conditional Statement, if visible, 1, else 0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Because we have the total number of cells (not shown), and we have the number of cells that belong to both 0 (non-visibility) and 1 (visibility), we can calculate the percentages of both by non-visibility/total and visibility/total. As shown, about 17% of the cells are visible, while the remaining 83% of cells are not visible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri3.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Umayri Wall, Indicated by the Red Ring&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;more-raster-calculation&#34;&gt;More Raster Calculation&lt;/h2&gt;

&lt;p&gt;When considering viewsheds, a useful raster calculation is combining the viewshed of different points. Returned data is visualized in such a way that you can see what areas are visible from both locations, or either location. So that is what we will do, using site2 (the green star). In this example, the magenta areas can be seen from the green star, the blue from the red star, the purple from both stars, and the white from neither.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri5.jpg&#34; alt=&#34;alt text&#34; title=&#34;Combined Visibility from the Green and Red Stars&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-impact-of-combined-viewsheds&#34;&gt;The Impact of Combined Viewsheds&lt;/h2&gt;

&lt;p&gt;Tying back into the original application domains, to get full view of a prison yard, towers can be added (or raised). To provide full coverage to a city, cellular towers can be added. And to gain &amp;lsquo;full control&amp;rsquo; over ancient sites, they too could put up more towers. So we will pretend they did, and we will see how much more visibility these added towers gave them. Our added towers will be indicated with white stars. Their respective viewsheds are indicated by the varying colors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri6.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Viewshed From Umayri with 6 Added Towers&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To get percentages of area covered, we can use the conditional statement from earlier. Remember that non-visible areas will be represented as 0&amp;rsquo;s and visible areas will be represented as 1&amp;rsquo;s. With some simple division, we can see that the new viewshed is far better, with 53% of the area being visible (as opposed to 17%), and 47% of the area not being visible (as opposed to 83%).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri7.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Viewshed from Umayri with 6 Added Towers&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Though this is a significant improvement, it is far from ideal. The towers were added at arbitrarily chosen sites (green points). This step in the analysis could have been optimized for the best visibility, with more emphasis on western sites than eastern, or more emphasis on high-elevation points. This simple analysis can be of tremendous value to any problem concerning visibility.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.casa.arizona.edu/MPP/viewshed/vspaper.html&#34;&gt;* Visibility Analysis and Ancient Settlement Strategies&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Working Example of a Conceptual Model</title>
      <link>https://ckhoward.github.io/blog/a-working-example-of-a-conceptual-model/</link>
      <pubDate>Tue, 21 Feb 2017 23:12:45 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/a-working-example-of-a-conceptual-model/</guid>
      <description>

&lt;p&gt;&lt;em&gt;A Working Example of a Conceptual Model&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An Object/Action Analysis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Conceptual models are all about representation. And the way something is represented is important, both when understanding something, and when creating something. When attempting to understand a thing, we try to make sure our conceptual models aren’t faulty, so we practice logic and critical thinking. Else, biases or fallacious thoughts may steer us in a wrong direction while we experience a discussion with somebody, and miscommunications may happen, and/or the discussion is derailed. Likewise, in an early phase of building a product, we flesh out our conceptual model of that product, so as to not be steered in a wrong direction. We avoid adding unneeded features early. We explicate the most essential features and stay focused on them, that way we can get a clear picture of the relationships between our objects and their functionalities, if multiple actions in an app can use the same functionality.&lt;/p&gt;

&lt;p&gt;Defining a conceptual model early will inform UI design in a way that avoids redundancy and it can keep people on track, so they don’t design and implement unneeded features. And when the design phase starts, it will be more efficient. Prototyping can begin earlier.&lt;/p&gt;

&lt;p&gt;Below is an example of a hypothetical Object/Action analysis before designing an email system. I am posting this because there are not a great number of examples online for this kind of thing. This example will consist of a first version and a final version; in our first versions of these models, it is very easy to add excessive features and to needlessly complicate things. For this reason, as with most works, it is often necessary to go back and revise. Cut. Cut. Cut. Until the conceptual model is as simple as can be and represents the absolute bare-bones of the application you have in mind, trim all excess.&lt;/p&gt;

&lt;h3 id=&#34;first-draft-flawed&#34;&gt;First Draft (Flawed):&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Objects&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Attributes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mailbox&lt;/td&gt;
&lt;td&gt;Owner, Current Focus, Inbox,   Starred, Sent, Draft, Spam, Trash&lt;/td&gt;
&lt;td&gt;Examine, Compose, Delete, Mark as   Read, Search, Archive&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Mail Message&lt;/td&gt;
&lt;td&gt;Sender, Recipient, Date, Subject,   Content, Attachment Description (if any)&lt;/td&gt;
&lt;td&gt;Compose, Delete, Print, Reply, Forward, Report, Block&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As noted, the first version is often flawed, and should be iterated on. The second table illustrates edits that were made to the first table.&lt;/p&gt;

&lt;h3 id=&#34;final-draft&#34;&gt;Final Draft:&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Objects&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Attributes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Operations&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mailbox&lt;/td&gt;
&lt;td&gt;User, Current Focus, &lt;s&gt;Inbox, Starred, Sent, Draft, Spam, Trash&lt;/s&gt;, +Mail Message&lt;/td&gt;
&lt;td&gt;Examine, &lt;s&gt;Compose, Delete, Mark   as Read&lt;/s&gt;, Search, &lt;s&gt;Archive&lt;/s&gt; +Organize&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Message&lt;/td&gt;
&lt;td&gt;&lt;s&gt;Sender, Recipient&lt;/s&gt;, +User, Date, Subject,   Content, Attachment Description (if any)&lt;/td&gt;
&lt;td&gt;&lt;s&gt;Compose&lt;/s&gt;, +Send, Delete, Print, &lt;s&gt;Reply,   Forward, Report, Block&lt;/s&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;User&lt;/td&gt;
&lt;td&gt;Name, Address&lt;/td&gt;
&lt;td&gt;Examine, Message, Add, Remove&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Mailbox:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For Attributes, I’ve crossed out Inbox, Starred, Sent, Draft, Spam, and Trash because they are not fundamental to the object and operations. I also replaced ‘Owner’ with ‘User’ as it interacts better with the Mail Message object (describing creator and receiver of messages). I also added Mail Message because they are contained (fundamentally) within the mailbox. I’ve added Organize to the operations because it is a verb that summarizes multiple actions: Delete is organizing into a Trashcan, Archiving is organizing, and placing into Spam and Drafts are also both organizing actions. I removed Compose because it is not essential to the mailbox itself; I only considered it an operation because there is a compose button in the CatMail interface, in other words, because of an implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;s&gt;Mail&lt;/s&gt; Message (to be concise):&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For attributes, I’ve removed Sender and Recipient and replaced both with User. For Operations, I removed Compose, Reply, and Forward; Send is an operation that condenses all of these actions, that isn’t implementation-based. Forward and Block were removed because they do not have a fundamental link to messages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;User:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For attributes, I’ve added Name and Address because they are user properties. For operations, I added Examine, Message, Add, and Delete because these simple terms can represent addition to lists (like report or block, reply list, forward list, etc.), and users are obviously attached to messages (and the mailbox is owned by the user, too).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Your Education at Random</title>
      <link>https://ckhoward.github.io/blog/your-education-at-random/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/your-education-at-random/</guid>
      <description>

&lt;h2 id=&#34;a-160-hour-1-500-4-000-mistake&#34;&gt;A 160-hour, $1,500-4,000 mistake.&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;re picking classes for next semester. You have four selected, and you&amp;rsquo;re now looking for a fifth. There are three options. How do you know which to pick? They&amp;rsquo;re equally interesting, albeit different routes, and they have different professors. The fact of the matter is, you have little information to base your choice off of. Likely no syllabus. Likely no knowledge of the professor. And likely no real insights.&lt;/p&gt;

&lt;h2 id=&#34;there-are-really-only-a-few-routes-to-try-to-get-more-information-in-these-regards&#34;&gt;There are really only a few routes to try to get more information in these regards.&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Visit &lt;em&gt;Rate my Professor&lt;/em&gt;: This is a weak option. The site has declined in popularity and has a word-count to keep posts concise, which basically leads to &amp;lsquo;it was good&amp;rsquo;-esque responses from students. There is no room for specificity. There is no room for discussion.&lt;/li&gt;
&lt;li&gt;Ask around: This is not a viable option. It&amp;rsquo;s incredibly inefficient, and even if you manage to find somebody that had a particular class, the lack of a relationship and the spontaneity of the question will likely result in a poorly- and rashly-thought out response. I think people would articulate their experiences better in an online platform, devoid of awkward social encounters.&lt;/li&gt;
&lt;li&gt;Request a syllabus: I don&amp;rsquo;t know why syllabi aren&amp;rsquo;t more accessible. I&amp;rsquo;ve often wanted to see what would be covered in a class, only to not be able to find a syllabus. And I&amp;rsquo;ve had experiences where I have emailed a professor multiple times for one, only for no response. Then I&amp;rsquo;ve even gone to the program adviser to get a copy of a course syllabus, which was also not helpful. Still, even if I did get one, there is a great deal of information that would be lacking. Learning experiences (and teaching styles) tend to be more nuanced than syllabi content.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is why we need a new site to facilitate &lt;em&gt;insightful&lt;/em&gt; discussions about courses.&lt;/p&gt;

&lt;h2 id=&#34;what-do-i-mean-by-insightful&#34;&gt;What do I mean by insightful?&lt;/h2&gt;

&lt;p&gt;Maybe a particular student took Course &amp;lsquo;A&amp;rsquo; alongside Course &amp;lsquo;H&amp;rsquo;. This student found that the courses complemented each other unexpectedly, and extraordinarily. She was able to build parallels between two disciplines, and find much more meaning in both subjects. This information would surely be beneficial to students selecting their classes for the next semester.&lt;/p&gt;

&lt;p&gt;Or maybe one professor talks very quietly and has illegible handwriting. Some professors assign homework and classwork that don&amp;rsquo;t seem to have any real-world application, discouraging students from pursuing the field further. If a professor killed this student&amp;rsquo;s dreams with their choices in assignments, that might be significant information; it could cause a student to change major (and likely, career), but it could also be a tremendous waste of time and money (yay, tuition!)&lt;/p&gt;

&lt;p&gt;You will always get insubstantial reviews. Students will irrationally pin blame on professors, failing to take responsibility for their own lack of effort or whatever. You might even get advertisements of some sort: Take this CLASS with PROFESSIRE MIRANDA, you&amp;rsquo;ll have your dream career in ONE MONTH!! But people can use their intuition to judge the quality of a review for themselves. Given the advent of spam filters and verification methods, this is trivial. Less trivial, schools could try to sabotage each other&amp;rsquo;s programs.&lt;/p&gt;

&lt;h2 id=&#34;implementation&#34;&gt;Implementation.&lt;/h2&gt;

&lt;p&gt;The site would be relatively simple. Create a database of schools. Allow somebody to post a review about a program or class for a particular school. If somebody is interested in a program or class that has yet to be posted, they can create a request. A school might be inclined to use alumni mailing-lists to try to fulfill a request (though admittedly, this might ignore important groups, like those that switched programs, or dropped out). The community can respond to different reviews; it would be useful to have the ability to ask more questions, or to agree/disagree with a post, or to give relevant feedback (like a professor switching school).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; the core functionality is post, request, and discuss. +1 for allowing anonymity.&lt;/p&gt;

&lt;h2 id=&#34;would-the-site-be-successful&#34;&gt;Would the site be successful?&lt;/h2&gt;

&lt;p&gt;There is a natural incentive for universities to subtly promote a website like this—if you have a great neuroscience program, wouldn&amp;rsquo;t you want the world to know? That would be a great way to instill confidence in students to travel and pay ridiculous out-of-state tuition. It would then be beneficial for students and the school itself to not force the hand of students in completing these sorts of class/professor-reviews. End-of-grade bonus points would get the participation of students that may not have anything valuable or insightful to contribute. It&amp;rsquo;s best if the students post &lt;em&gt;if they want to&lt;/em&gt;, if they really feel compelled. Of course, that requires knowledge of the site&amp;rsquo;s existence, and a student that wants to participate. I&amp;rsquo;m sure many wouldn&amp;rsquo;t want to share their experiences, even if others would find them useful. I don&amp;rsquo;t believe a site like this would be difficult to market. It would just need a mission statement that students can identify and connect with. Like not leaving education up to chance, by making informed decisions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terrorist Targets [Exploratory]</title>
      <link>https://ckhoward.github.io/blog/terrorist-targets-exploratory/</link>
      <pubDate>Tue, 03 Jan 2017 16:46:12 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/terrorist-targets-exploratory/</guid>
      <description>&lt;p&gt;If you were to start some Google searches for terrorist attacks, while specifying the terrorist organization (e.g. Boko Haram, or the Lord&amp;rsquo;s Resistance Army), you will likely see different themes within the listed results. Going to a page that aggregates LRA attacks, you get:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/LRA aggregation.png&#34; alt=&#34;alt text&#34; title=&#34;lracrisistracker.com&#34; /&gt;&lt;/p&gt;

&lt;p&gt;and for Boko Haram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/bokoharamv2.png&#34; alt=&#34;alt text&#34; title=&#34;breakingnews.com - sadly shut down 12/31/16&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Assuming these sources weren&amp;rsquo;t cherry-picked, we might infer that the LRA tends to attack people and their property, with assault rifles, while Boko Haram might be more likely to attack crowded public places, with explosives.&lt;/p&gt;

&lt;p&gt;It would be interesting to see how accurately media reporting reflects actual data. The &lt;a href=&#34;http://www.start.umd.edu/data-tools/global-terrorism-database-gtd&#34;&gt;Global Terrorism Database&lt;/a&gt; provides a large dataset that we can use to explore attacks, containing variables like terrorist organization involved, location, target, means of attack (like weapon detail), and many others. This database will give us the means of exploring terrorism, particular terrorist organizations, and their similarities and differences. Providing this context for particular groups, we can see the proportions of targeted groups. It mostly comes down to private citizens and &lt;em&gt;the establishment&lt;/em&gt; (business, military, police, and government).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/target_frequency.png&#34; alt=&#34;alt text&#34; title=&#34;Frequency of Attack on Indicated Target&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So these are the primary targets in terrorist attacks; how do individual groups play into this? To compare relatively diverse groups, we can factor in different regions, or groups within different affiliations and alliances (e.g., some branches may be associated with the Islamic State, and some may not be).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/targets.png&#34; alt=&#34;alt text&#34; title=&#34;Terrorist Targets&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/attack_type.png&#34; alt=&#34;alt text&#34; title=&#34;Terrorist Attack Types&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Between these two charts, we can see striking differences in who they attack and how they carry out the attack, possibly pointing to different objectives, structures of leadership, and resources. The data reflects what the media observed earlier: the LRA kidnaps and assaults citizens, and Boko Haram uses explosives frequently, though they do still favor armed assaults. But why is there such a significant difference between the Kurdistan Workers&amp;rsquo; Party (the PKK), notably attacking the establishment, while the LRA attacks people? These are rhetorical questions—answers exist, but they are outside the scope of this post.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Considering Objectives:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When looking at high levels of hostage taking (kidnapping) and armed assault by the LRA, this may be an indicator that they use these means as a primary &lt;a href=&#34;http://www.iiss.org/en/topics/terrorism/how-kidnapping-funds-terrorism-d704&#34;&gt;source of income&lt;/a&gt;. The logs above show citizens&amp;rsquo; homes being looted. We could look further to see if the noted abductions have ransoms attached, or if it is primarily means of recruitment. Since Boko Haram&amp;rsquo;s assaults are comparable to the LRA&amp;rsquo;s, it may be interesting to show whether or not Boko Haram&amp;rsquo;s tendencies shifted after splitting; part went into ISIL&amp;rsquo;s West Africa branch, while the other joined an al-Qaida branch. If these attacks are the LRA&amp;rsquo;s source of income, it is non-sustainable. If the primary objective isn&amp;rsquo;t gains in income and recruitment, and it is to establish power to rule over citizens, that is another story.&lt;/p&gt;

&lt;p&gt;The PKK on the otherhand primarily attacks military, police, business, and government targets. It makes more sense that these attacks are politically motivated rather than financially motivated.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An Alternative Possibility:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The motivations may not correlate well with actions. Explosives are commonly used, so even if a target is a military convoy or building, there may be civilian casualties within range of the explosion. A terrorist organization&amp;rsquo;s leadership may intend to carry out a strategy for acheiving some goal, but if leadership has little control over its followers, attacks may be carried out that do not follow strategy. This could prompt further investigation into the structures of these terrorist organizations, whether they&amp;rsquo;re more network-like, or more heirarchical. Both have their own &lt;a href=&#34;https://www.hsaj.org/articles/122&#34;&gt;pros and cons&lt;/a&gt;, but these structures can have significant influences on recruitment, efficiency of attacks, safety of its members, and more.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>this is a no-handed post</title>
      <link>https://ckhoward.github.io/blog/this-is-a-no-handed-post/</link>
      <pubDate>Tue, 03 Jan 2017 16:46:12 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/this-is-a-no-handed-post/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>