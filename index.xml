<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chris Howard</title>
    <link>https://ckhoward.github.io/index.xml</link>
    <description>Recent content on Chris Howard</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 03 Jan 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://ckhoward.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Less Confusing Tutorial on Jupyter (IPython) Widgets</title>
      <link>https://ckhoward.github.io/blog/a-less-confusing-tutorial-on-jupyter-ipython-widgets/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/a-less-confusing-tutorial-on-jupyter-ipython-widgets/</guid>
      <description>

&lt;h1 id=&#34;a-less-confusing-tutorial-on-jupyter-ipython-widgets&#34;&gt;A Less Confusing Tutorial on Jupyter (IPython) Widgets&lt;/h1&gt;

&lt;p&gt;Jupyter widgets are an awesome tool for creating interactive dashboards, but documentation can be a little excessive if you&amp;rsquo;re just looking for basic functionality. It really doesn&amp;rsquo;t have to be so complicated.&lt;/p&gt;

&lt;h2 id=&#34;our-widget-use-case&#34;&gt;Our widget use-case&lt;/h2&gt;

&lt;p&gt;My data visualization team wanted to provide researchers with a GUI for visualizing species distributions, and we wanted to give them the power and flexibility to specify different parameters. These parameters included species of interest, the predictive algorithm of interest, predictive power (or threshold), and the speed at which the movements of the species over the course of a year were animated.&lt;/p&gt;

&lt;h2 id=&#34;the-simplicity-of-widget-code&#34;&gt;The simplicity of widget code&lt;/h2&gt;

&lt;p&gt;Creating the GUI elements for these varying parameters is as simple as importing the required libraries, creating a variable, assigning a widget to that variable, and then displaying that variable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Import requirements
import ipywidgets as wg
from IPython.display import display

# Set initial style of the interactive GUI.
style = {&#39;description_width&#39;: &#39;initial&#39;}

# Initialize the elements of the GUI.
name = wg.Text(value=&#39;Name&#39;, description=&amp;quot;Species: &amp;quot;, style=style)
algo = wg.Dropdown(options = [&amp;quot;CTA&amp;quot;, &amp;quot;GLM&amp;quot;, &amp;quot;RF&amp;quot;],
 description=&amp;quot;Algorithm: &amp;quot;, style=style)
threshold = wg.Dropdown(options = [&#39;1&#39;, &#39;10&#39;, &#39;50&#39;],
 description=&amp;quot;Prediction Threshold: &amp;quot;, style=style)
month = wg.SelectionSlider(description=&amp;quot;Month: &amp;quot;, options=[&#39;jan&#39;, &#39;feb&#39;, &#39;mar&#39;, &#39;apr&#39;, &#39;may&#39;, &#39;jun&#39;, &#39;jul&#39;, &#39;aug&#39;, &#39;sep&#39;, &#39;oct&#39;, &#39;nov&#39;, &#39;dec&#39;, &#39;all&#39;], style=style)
speed = wg.FloatSlider(value=.5,min=0,max=1.0,step=0.1,description=&#39;Animation Speed:&#39;,style=style)

# Display the GUI.
display(name, algo, threshold, month, speed)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note, variables like &lt;code&gt;speed&lt;/code&gt; might not make a great deal of sense right now, but just focus on the functions&amp;rsquo; arguments. Speed&amp;rsquo;s default value is set to .5, and has a range of [0, 1] and steps at 0.1. The &lt;code&gt;description&lt;/code&gt; argument is the widget&amp;rsquo;s label that the user sees.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/widget.jpg&#34; alt=&#34;Widget&#34; title=&#34;IPython GUI Widget&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Just like that, the &lt;code&gt;display&lt;/code&gt; statement takes all of those widget variables and shows them off in the Jupyter Notebook. Now the researchers have more power in choosing what to visualize and how to visualize it.&lt;/p&gt;

&lt;h2 id=&#34;widget-intuition&#34;&gt;Widget intuition&lt;/h2&gt;

&lt;p&gt;So, how do you use it? In my experience, users are inclined to change the values around as appropriate, and then execute that cell containing the GUI. Widgets do not operate like this. The variables are active and responsive; the variable holds whatever value is set in the GUI (like CTA for Algorithm), and does not require cell execution. Cell execution resets the GUI and the variable values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/widget2.jpg&#34; alt=&#34;Widget 2&#34; title=&#34;IPython GUI Widget Inputs&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In changing the GUI around in this way, the variable &lt;code&gt;name&lt;/code&gt; now has a value of &amp;lsquo;Papilio_glaucus;&amp;rsquo; &lt;code&gt;algo&lt;/code&gt; has &amp;lsquo;CTA;&amp;rsquo; &lt;code&gt;threshold&lt;/code&gt; has &amp;lsquo;10&amp;rsquo; and so forth. These variables can now be used in other code.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Import requirements
import os
import matplotlib.pyplot as plt
from PIL import Image as im

# Initialize the filename for each SDM.
filename = &amp;quot;./SDMs/&amp;quot; + name.value + &amp;quot;/&amp;quot; + month.value + &amp;quot;/&amp;quot; + algo.value + &amp;quot;-&amp;quot; + threshold.value + &amp;quot;.png&amp;quot;

# Check for file existence and then display the map (with a title).
if os.path.isfile(filename):
    print(&#39;----------------------------------------&#39;)
    print(&#39;Species: &#39;, name.value)
    print(&#39;Algorithm: &#39;, algo.value)
    print(&#39;Prediction Threshold: &#39;, threshold.value)
    print(&#39;Month: &#39;, str(month.value))
    print(&#39;----------------------------------------&#39;)
    img = im.open(filename)
    fig = plt.figure()
    fig.set_size_inches(14, 8, forward=True)
    fig.suptitle(name.value + &#39; - &#39; + month.value + &#39; - &#39;+ algo.value + &#39; - &#39; + threshold.value)
    plt.imshow(img)
    plt.show(img)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/widget_output.jpg&#34; alt=&#34;Widget Output&#34; title=&#34;Widget Output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If the user desires, they can use the speed variable to change the speed at which the species distributions are animated (or in other words, how quickly they move across the map). This is all controlled by basic GUI objects: one textbox, two dropdowns, and two sliders. Of course, the way these variables are used will differ between projects (not all code is provided here).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/50.gif&#34; alt=&#34;Widget Animation Speed&#34; title=&#34;Widget Animation&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;widget-gotchas&#34;&gt;Widget gotchas&lt;/h2&gt;

&lt;p&gt;In my experience, Jupyter widgets don&amp;rsquo;t play nicely with Github and nbviewer when it comes to simply viewing the GUI objects. It isn&amp;rsquo;t rendered properly. Here is an example of how the above widgets look in Github:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/broken.jpg&#34; alt=&#34;Broken&#34; title=&#34;Incorrect Widget Rendering&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Maybe this will be fixed in the future. In the meantime, Github or nbviewer presentations will need to be replaced with live demos—&lt;em&gt;the horror&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;the-other-great-contributors-responsible-for-the-example-code&#34;&gt;The other great contributors responsible for the example code&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/agarwalmegha6&#34;&gt;Megha Agarwal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/trillsauceda&#34;&gt;Isaiah Sauceda&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Deploying a Tor Hidden Service with DigitalOcean</title>
      <link>https://ckhoward.github.io/blog/deploying-a-tor-hidden-service-with-digitalocean/</link>
      <pubDate>Fri, 29 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/deploying-a-tor-hidden-service-with-digitalocean/</guid>
      <description>

&lt;h1 id=&#34;deploying-a-tor-hidden-service-with-digitalocean&#34;&gt;Deploying a Tor Hidden Service with DigitalOcean&lt;/h1&gt;

&lt;h4 id=&#34;a-quick-primer-on-tor-hidden-services&#34;&gt;A quick primer on Tor hidden services&lt;/h4&gt;

&lt;p&gt;The Tor protocol picks 3 onion routers at random, builds &lt;em&gt;circuits&lt;/em&gt; to them, treating them as introduction (or contact) points; since this is a circuit, there is no direct connection, so no IP addresses are shared, just public keys. These public keys work toward &amp;ldquo;shaking hands&amp;rdquo; at a rendezvous point. If the handshake happens, the rendezvous point connects Alice&amp;rsquo;s and Bob&amp;rsquo;s circuits, and then they can communicate anonymously. While the protocol isn&amp;rsquo;t without flaw, it is still powerful, especially when used in conjunction with other practices and tools. If you have sensitive information, it is your responsibility to ensure that your system is protected, and that your visitors are protected. Deploying a hidden service is a good step toward offering that protection.&lt;/p&gt;

&lt;h2 id=&#34;deploy-the-vps-with-digital-ocean&#34;&gt;Deploy the VPS with Digital Ocean&lt;/h2&gt;

&lt;p&gt;We are going to create a Debian Droplet on DigitalOcean. After creating my account, I fork up the $5 to create my Droplet—it would be cool if DigitalOcean gave a student discount for students to practice, but I digress.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/vps_install_1.jpg&#34; alt=&#34;VPS Install&#34; title=&#34;VPS Install&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I chose not to add an SSH key, and I left the hostname at its default. If you are comfortable with SSH, feel free to add yours, but this tutorial assumes a lack of familiarity. Click that giant green &lt;strong&gt;Create&lt;/strong&gt; button and the droplet should be created within the minute.&lt;/p&gt;

&lt;h3 id=&#34;set-up-ssh-for-securely-logging-in&#34;&gt;Set Up SSH for Securely Logging In&lt;/h3&gt;

&lt;p&gt;DigitalOcean has a solid tutorial on first steps for &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-configure-ssh-key-based-authentication-on-a-linux-server&#34;&gt;setting up SSH&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I generate a key and since I already have a (useless) key named id_rsa, I choose to overwrite it. I hit enter twice to get through the passphrase prompts. The key is generated (and some parts blanked out by me)—this generated randomart is supposed to make it easier for the user to verify that the key hasn&amp;rsquo;t been tampered with, otherwise they would have to compare long strings of text.&lt;/p&gt;

&lt;p&gt;Because we created the droplet without an SSH key, DigitalOcean sent us an email with our username (likely root), droplet IP, and temporary password. I personally logged into the console through the DigitalOcean web-portal and changed my password to something else, but that isn&amp;rsquo;t necessary.&lt;/p&gt;

&lt;p&gt;To copy your public key to your server Use &lt;code&gt;ssh-copy-id username@ip&lt;/code&gt;. Note, your username will be root by default, counterintuitively. Use either the temporary password that was emailed to you, or your updated password if you went that route. Now you should be able to SSH into your server with &lt;code&gt;ssh username@ip&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After successfully SSHing into your server, consider disabling password authentication on your server to prevent brute-force attacks. In the linked tutorial, at the bottom, under &lt;strong&gt;Disabling Password Authentication on your Server:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Change from &lt;code&gt;PasswordAuthentication yes&lt;/code&gt; to &lt;code&gt;PasswordAuthentication no&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo service ssh restart&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;img/ufw5.jpg&#34; alt=&#34;Password Authentication&#34; title=&#34;Password Authentication Off&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now the only way to get into your server is SSH.&lt;/p&gt;

&lt;h3 id=&#34;install-the-uncomplicated-firewall&#34;&gt;Install the Uncomplicated Firewall&lt;/h3&gt;

&lt;p&gt;ufw is a frontend for iptables, which is a firewall system for Linux, but simpler. ufw will be installed to at least reduce the number of attacks your server might get, as we are going to block most traffic. Chances of attack are pretty high, you can install &lt;a href=&#34;https://www.fail2ban.org/wiki/index.php/Main_Page&#34;&gt;Fail2Ban&lt;/a&gt; to scan and ban IP addresses associated with malicious activity (most of which will probably be Chinese). It&amp;rsquo;s better to get security stuff right early on. Also be mindful that Tor relays and exit nodes are also frequently attacked.&lt;/p&gt;

&lt;p&gt;Run the following commands in your command line, while logged into your server:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get upgrade -y
sudo apt-get dist-upgrade -y
sudo apt-get install ufw
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Updating first will help you prevent running into this error:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Package ufw is not available, but is referred to by another package. This may mean that the package is missing, has been obsoleted, or is only available from another source&lt;/p&gt;

&lt;p&gt;E: Package &amp;lsquo;ufw&amp;rsquo; has no installation candidate&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;and you should be good to go. The firewall is installed. Now enable the firewall:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo ufw enable -y
sudo ufw status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Status &lt;em&gt;should&lt;/em&gt; output &lt;strong&gt;Status: active&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;create-a-new-user-account&#34;&gt;Create a New User Account&lt;/h3&gt;

&lt;p&gt;To be careful not to do something disastrous in the server, it is good practice to create a new, non-root user account, and to limit its privileges so that we reduce the likelihood of breaking something important. Additionally, using an alternative account and getting rid of the root account can also work toward preventing being brute-forced. Again, DigitalOcean has a nice tutorial on &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-16-04&#34;&gt;creating a new user&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;adduser ckhoward #but enter your own username, of course
usermod -aG sudo ckhoward
ssh-copy-id ckhoward@ip #this works since we already generated the key
su - ckhoward #to switch to other user
chmod 700 ~/.ssh
nano .ssh/authorized_keys #this is where you paste the contents of your id_rsa pub file, cntrl x, y, ENTER
chmod 600 ~/.ssh/authorized_keys
exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course, test that all of this is working by &lt;code&gt;ssh ckhoward@ip&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;install-apache-web-server&#34;&gt;Install Apache Web server&lt;/h3&gt;

&lt;p&gt;DigitalOcean provides an alternative tutorial on &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-the-apache-web-server-on-ubuntu-16-04&#34;&gt;installing the apache web server&lt;/a&gt;. A command is needed to let the firewall know that apache is okay. This is referred to as a rule. &lt;code&gt;sudo ufw allow from 127.0.0.1 to any port 80&lt;/code&gt; You can find your own IP address and only allow that, if you want, while you set up your server. This would have to be altered later on, if you expect others to hop on. If you would like to look at the rules you&amp;rsquo;ve set for allowing and denying connections, &lt;code&gt;sudo ufw status numbered&lt;/code&gt; is a handy command. At this point, it should output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[1] 80 ALLOW IN 127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In my case, I set a rule for allowing only SSH connections from my IP address.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/ufw7.jpg&#34; alt=&#34;UFW Firewall Rules&#34; title=&#34;Firewall Rule&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Installing apache2 and running it is trivial, enter the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install apache2
sudo systemctl start apache2.service
sudo systemctl status apache2
#if apache is running, enter:
hostname -I # this will give you a couple of IPs, enter them into your browser to find your site
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After you&amp;rsquo;ve accessed your site from your browser, your activity should have been logged. You can check this with by finding and printing your logs with &lt;code&gt;tail -f /var/log/apache2/access.log&lt;/code&gt;. There will be information associated with the request, the browser used, and the timestamp.&lt;/p&gt;

&lt;h3 id=&#34;install-tor&#34;&gt;Install Tor&lt;/h3&gt;

&lt;p&gt;Run the command &lt;code&gt;sudo apt-get install tor&lt;/code&gt;. Surprisingly simple. The Tor documentation instructs anyone deploying a hidden service to navigate to the torrc config file and uncomment the two statements:
&amp;gt;#HiddenServiceDir /var/lib/tor/hidden_service/&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;#HiddenServicePort 80 127.0.0.1:80&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thus, we find the config file and edit it with the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;find / -iname &amp;quot;torrc&amp;quot;
sudo nano /etc/tor/torrc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the apache2 service is still running, let&amp;rsquo;s just restart that with &lt;code&gt;sudo systemctl restart apache2.service&lt;/code&gt;. Start Tor with &lt;code&gt;sudo service tor start&lt;/code&gt;. Tor is now running and has an associated Onion address. To find this address, run &lt;code&gt;cat $HOME/hidden_service/hostname&lt;/code&gt; and the Onion address will print. This guide has set you up for the basics of getting a hidden service running with DigitalOcean. There are other guides on better practices for &lt;a href=&#34;https://riseup.net/en/security/network-security/tor/onionservices-best-practices&#34;&gt;securing hidden services&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/hidden_service.jpg&#34; alt=&#34;Hidden Service Result&#34; title=&#34;Resulting Onion Site&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit goes to &lt;a href=&#34;http://u.arizona.edu/~dsidi/&#34;&gt;David Sidi&lt;/a&gt; for coming up with this idea in the form of a class assignment.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to the Stylo Library [R]</title>
      <link>https://ckhoward.github.io/blog/an-introduction-to-the-stylo-library-r/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/an-introduction-to-the-stylo-library-r/</guid>
      <description>

&lt;h1 id=&#34;an-introduction-to-the-stylo-library&#34;&gt;An Introduction to the Stylo Library&lt;/h1&gt;

&lt;h3 id=&#34;what-is-stylometry&#34;&gt;What is Stylometry?&lt;/h3&gt;

&lt;p&gt;Stylometry uses linguistic style to determine who authored some anonymous piece of writing, and it has diverse applications. The authorship of some suicide notes may be questionable. Most forum users have aliases in an attempt to anonymize themselves. And some authors publish their writings under pseudonyms. In these varying cases, stylometry can be used to deanonymize an author.&lt;/p&gt;

&lt;h3 id=&#34;what-is-stylo&#34;&gt;What is Stylo?&lt;/h3&gt;

&lt;p&gt;Stylo is a library for the R programming language that is used for conducting stylometric analyses. Stylometry sounds intimidating, but this library makes these linguistic analyses so simple that its users do not have to be computational linguists. Stylo isn&amp;rsquo;t programmatically intensive and mostly just requires a particular naming convention for files and directories. If users are uncomfortable with writing out functions with varying arguments, they can opt to use a simple GUI.&lt;/p&gt;

&lt;h3 id=&#34;a-stylometric-analysis-of-run-the-jewels&#34;&gt;A Stylometric Analysis of Run the Jewels&lt;/h3&gt;

&lt;p&gt;Run the Jewels is a group of two rappers, El-P and Killer Mike. In collaborative songs, most assume that each rapper raps the verses that they write. This is an interesting example that can be used to show how the Stylo library works for authorship attribution. Models will be trained on lyrics of each rapper when they worked independently, and will then be fed Run the Jewels lyrics to get an idea of each artist&amp;rsquo;s creative footprint in a song, and how collaboration might work. This analysis will serve as a concrete example for some of the uses and quirks of Stylo.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-data&#34;&gt;Getting the data&lt;/h2&gt;

&lt;p&gt;To determine if the project was viable—or that each artist had a distinctive style—I had to get the data. In this case, the data is lyrics that are added to a corpus. Using Enrico Bacis&amp;rsquo; &lt;a href=&#34;https://github.com/enricobacis/lyricwikia&#34;&gt;LyricWikia API&lt;/a&gt; to get lyrics, I use a Bash loop that iterates over an array of hardcoded song names to save the lyrics into the corpus.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/bash elp.png&#34; alt=&#34;Bash Array&#34; title=&#34;The Bash Array&#34; /&gt;
&lt;img src=&#34;img/lyrics elp.png&#34; alt=&#34;Corpus&#34; title=&#34;The Corpus&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The Stylo R library is heavily reliant on naming conventions, both for directories and files. Everything before an underscore is treated as a class to be analyzed, so if you are looking at texts of George R.R. Martin, you would do something like GeorgeRRMartin_ASongofIceandFire and the software will understand that George R.R. Martin is an author of interest, responsible for that work. I did the same with El-P and Killer Mike, otherwise each author would not have been treated as unique.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/fixed_formatting.jpg&#34; alt=&#34;New Structure&#34; title=&#34;Changing the naming conventions&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;project-viability-exploratory-analysis&#34;&gt;Project Viability / Exploratory Analysis&lt;/h2&gt;

&lt;p&gt;Now that we have the data, we can do a bit of exploration to check whether the project is viable, in this case, whether each rapper has a distinctive style. This can be achieved with a principal components analysis. Since the file naming convention has been altered for Stylo use, the directory naming still has to be altered, and then the function will work when called.&lt;/p&gt;

&lt;h3 id=&#34;the-stylo-function&#34;&gt;The stylo() function&lt;/h3&gt;

&lt;p&gt;Intuitively enough, one of the most powerful functions in the Stylo library is &lt;code&gt;stylo()&lt;/code&gt;. Running this function will load the GUI and the users can then customize the analysis. The GUI that loads appears as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/gui1.jpg&#34; alt=&#34;Stylo Input&#34; title=&#34;Stylo GUI Input&#34; /&gt;
&lt;img src=&#34;img/gui2.jpg&#34; alt=&#34;Stylo Features&#34; title=&#34;Stylo GUI Features&#34; /&gt;
&lt;img src=&#34;img/gui3.jpg&#34; alt=&#34;Stylo Statistics&#34; title=&#34;Stylo GUI Statistics&#34; /&gt;
&lt;img src=&#34;img/gui4.jpg&#34; alt=&#34;Stylo Sampling&#34; title=&#34;Stylo GUI Sampling&#34; /&gt;
&lt;img src=&#34;img/gui5.jpg&#34; alt=&#34;Stylo Output&#34; title=&#34;Stylo GUI Output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We will go into detail about how the parameters work in an intermediate Stylo post, but for now, only directory information is important. The working directory must be set in R. For this example, it is &lt;code&gt;setwd(&amp;quot;E\\Stylometry\\Run the Jewels&amp;quot;)&lt;/code&gt;. The &lt;code&gt;stylo()&lt;/code&gt; function requires a directory called &lt;code&gt;corpus_files&lt;/code&gt; if it is being run without explicit arguments (i.e. as stylo()). This &lt;code&gt;corpus_files&lt;/code&gt; directory will contain all of the files that you want in your analysis, in this case, all songs by El-P and Killer Mike that I want within the PCA.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/corpus_files.jpg&#34; alt=&#34;Stylo corpus_files&#34; title=&#34;Stylo Corpus Files&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After running the function above, with the parameters shown, a PCA is generated. As shown, the library uses the naming convention to color-code each author. El-P is red and Killer Mike is green. It is evident that the rappers differ from each other stylistically when writing music, so the project seems viable. We can proceed to using some of the more interesting tools that Stylo offers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/rtj pca.png&#34; alt=&#34;PCA Visualization&#34; title=&#34;PCA Visualization&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;quick-recap&#34;&gt;Quick Recap&lt;/h3&gt;

&lt;p&gt;All that is needed to generate this PCA is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Proper naming of the directory/corpus&lt;/li&gt;
&lt;li&gt;Proper naming of files in the directory/corpus&lt;/li&gt;
&lt;li&gt;A simple one-line function call&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;using-the-svm-classification-algorithm&#34;&gt;Using the SVM Classification Algorithm&lt;/h2&gt;

&lt;p&gt;Stylo&amp;rsquo;s classification power is what lures most of its users. With other tools, building a classifier and cleaning the data takes several lines of code that aren&amp;rsquo;t too straight-forward, increasing the barrier of entry for those that aren&amp;rsquo;t computationally-inclined. As with the last function, the Stylo library makes this more simple for users, with properly named files and directories, it only takes a single line of code to build and use the classifier.&lt;/p&gt;

&lt;p&gt;There are multiple flavors of classification, but the two functions that we will look at are &lt;code&gt;classify()&lt;/code&gt; and &lt;code&gt;rolling.classify()&lt;/code&gt;. Again, we will get into the technical details in the intermediate post, as this is just a light primer.&lt;/p&gt;

&lt;h3 id=&#34;the-classify-function&#34;&gt;The classify() function&lt;/h3&gt;

&lt;p&gt;Two subdirectories must be present in the working directory for this function to work; &lt;code&gt;primary_set&lt;/code&gt; contains the training data that the classification model will learn from, and &lt;code&gt;secondary_set&lt;/code&gt; will work as the test set. For this Run the Jewels example, the primary_set directory contains 80% (44) of the songs in the total corpus (55), and the secondary_set will contain 20% (11) of the songs. Naturally, each rapper is equally sampled for both directories in order to avoid biasing the model.&lt;/p&gt;

&lt;p&gt;After running &lt;code&gt;classification &amp;lt;- classify()&lt;/code&gt;, a GUI will load allowing me to choose the function&amp;rsquo;s arguments, just like before. Assigning the function to a variable lets us access different values. As shown, from &lt;code&gt;classification$success.rate&lt;/code&gt;, the value of accurately-classified songs is returned, which is 81.8%.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/svm_accuracy.png&#34; alt=&#34;SVM Accuracy&#34; title=&#34;SVM Classification Accuracy&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The SVM model learned El-P&amp;rsquo;s and Killer Mike&amp;rsquo;s linguistic styles from the songs in the primary_set and applied that learning to the songs in the secondary_set, correctly classifying each of those 11 songs. This accuracy is mostly due to the strength of the samples, each song&amp;rsquo;s word count is enough to make the sample powerful. If the files in the directory were smaller, like individual verses, the accuracy is reduced.&lt;/p&gt;

&lt;h3 id=&#34;the-rolling-classify-function&#34;&gt;The rolling.classify() function&lt;/h3&gt;

&lt;p&gt;Two subdirectories must also be present in the working directory for this function to work; &lt;code&gt;reference_set&lt;/code&gt;, which will contain all of the samples that you want the model to learn from, and &lt;code&gt;test_set&lt;/code&gt;, which will contain the questionable or anonymous piece of writing that is of interest. Running the following code generates a visualization of classified samples, color-coded based on the naming convention used.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rolling.info &amp;lt;- rolling.classify(write.png.file = TRUE, classification.method = &amp;quot;svm&amp;quot;, mfw = 300, training.set.sampling = &amp;quot;normal.sampling&amp;quot;,
slice.size = 50, slice.overlap = 45,
milestone.label=c(&amp;quot;El-P&amp;quot;, &amp;quot;Killer Mike&amp;quot;, &amp;quot;El-P&amp;quot;, &amp;quot;Killer Mike&amp;quot;, &amp;quot;Zach de la Rocha&amp;quot;, &amp;quot;Killer Mike&amp;quot;))

title(main=&amp;quot;Run the Jewels \n A Report to the Shareholders / Kill Your Masters&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/RTJ_KYM.png&#34; alt=&#34;RTJ_KYM&#34; title=&#34;Rolling SVM Run the Jewels&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This song, &lt;em&gt;A Report to the Shareholders / Kill Your Masters&lt;/em&gt;, has about 1,000 words in it, as shown by the x-axis. The first 180 words or so are classified as El-P because of the red coloring. This function also has handy labeling functionality, where excerpts can be given vertical labels. For example, if I go into test_set and find the song file that I am interested in, &lt;em&gt;Kill Your Masters&lt;/em&gt;, and add the text &lt;code&gt;xmilestone&lt;/code&gt; before each of the verses, the argument &lt;code&gt;milestone.label&lt;/code&gt; assigns each value to each xmilestone. In the above song, there are six &lt;code&gt;xmilestone&lt;/code&gt; entries, and the function&amp;rsquo;s argument has six corresponding labels. This is useful for comparing classification to some area of interest; in this case, I know which verse belongs to which rapper, so I can manually insert those vertical labels to see if the classifications are &amp;ldquo;accurate.&amp;rdquo; The depth of the colored blocks indicate certainty. Where there are full blocks, there is the most certainty, but where there are dips, there is uncertainty.&lt;/p&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;We can see that some of Killer Mike&amp;rsquo;s lyrics are classified as being authored by El-P, and that Zach de la Rocha&amp;rsquo;s verses are also mostly classified as El-P (which makes sense, given the model did not learn de la Rocha&amp;rsquo;s style). The depth of the colored blocks indicate certainty, so it appears that most uncertainty is around verse transition, which could indicate that the two rappers tend to collaborate in this area, perhaps creating a bridge from one rapper&amp;rsquo;s verses, to the other&amp;rsquo;s, so that the song remains fluid. In general, it seems that the model could be biased towards El-P based on the fact that Stylo largely uses MFW (most frequent words).&lt;/p&gt;

&lt;p&gt;With a little tweaking of the filenames and directory names, a single function can be used to clean and classify the data. One line of code takes the place of the several lines that would be used for breaking the text down into the desired number of n-grams, the removal of pronouns, the proper format of text, the building of the model, the training and testing of the data, and even the visualizations. If interested in some of the technical details of how the library works, stay posted for the intermediate-level Stylo post.&lt;/p&gt;

&lt;h2 id=&#34;sources&#34;&gt;Sources&lt;/h2&gt;

&lt;p&gt;Stylo (R Library)
Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: a package for computational text analysis. &amp;ldquo;R Journal&amp;rdquo;, 8(1): 107-121.&lt;/p&gt;

&lt;p&gt;Rolling Stylometry
Maciej Eder; Rolling stylometry, Digital Scholarship in the Humanities, Volume 31, Issue 3, 1 September 2016, Pages 457–469, &lt;a href=&#34;https://doi.org/10.1093/llc/fqv010&#34;&gt;https://doi.org/10.1093/llc/fqv010&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;LyricWikia API (Python)
&lt;a href=&#34;https://github.com/enricobacis/lyricwikia&#34;&gt;https://github.com/enricobacis/lyricwikia&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FIXBUX: A Web-extension to Safeguard Users from Themselves [Javascript]</title>
      <link>https://ckhoward.github.io/blog/fixbux-a-web-extension-to-safeguard-users-from-themselves-javascript/</link>
      <pubDate>Tue, 12 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/fixbux-a-web-extension-to-safeguard-users-from-themselves-javascript/</guid>
      <description>

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;It would seem that people are becoming more conscious of behavior&amp;rsquo;s role in retaining, taking back, and even losing privacy. Privacy technology isn&amp;rsquo;t currently a solution to privacy issues, so it is becoming more important to consider how we interact with these technologies, and how we make decisions. We have a greater propensity to share when our friends share, when we trust the technology maintainers, and defaults surprisingly dictate the way we behave, so what happens when you opt-in, by default, to share your social network data with a host of third parties?&lt;/p&gt;

&lt;p&gt;There are certain contexts that we don&amp;rsquo;t think twice when sharing, like when looking for a job and we put our information up on job networking sites. It almost feels like a societal requirement sometimes, like you have to post your resume to these databases that then circulate it to a plethora of recruiters. This information often contains addresses, past employment, where you were, and through inference, possibly who you know and interact with.&lt;/p&gt;

&lt;p&gt;These sites are responsible for important user data, so how do they influence user behavior? Often with Bully UX. Elements of design can be used to pressure a user into doing something, and this is where Bully UX lies, right between &lt;em&gt;regular&lt;/em&gt; UX and &lt;em&gt;dark&lt;/em&gt; UX, where a user might feel inadequate in some way, for not having some quality (or at least not sharing it). A simple web-extension can execute a little bit of Javascript and help users turn a blind eye to these coercive elements.&lt;/p&gt;

&lt;p&gt;FIXBUX (Fix Bully UX) is a Chrome web-extension that removes coercive elements.&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Requirement:&lt;/strong&gt; Google Chrome&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;git clone https://www.github.com/ckhoward/fixbux.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Open Google Chrome&lt;/li&gt;
&lt;li&gt;Type &lt;code&gt;chrome://extensions&lt;/code&gt; into the URL bar&lt;/li&gt;
&lt;li&gt;Enable developer mode&lt;/li&gt;
&lt;li&gt;Load unpacked extension and select the FIXBUX/FIXBUX/ directory&lt;/li&gt;
&lt;li&gt;Enable the extension and hit &lt;code&gt;Control + R&lt;/code&gt; to reload&lt;/li&gt;
&lt;li&gt;Click the extension icon and visit your page of choice&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;case-studies&#34;&gt;Case studies&lt;/h2&gt;

&lt;h3 id=&#34;linkedin&#34;&gt;LinkedIn&lt;/h3&gt;

&lt;p&gt;LinkedIn initially inspired the idea for this extension, but since its conception, LinkedIn has actually improved their UI to be less coercive and better off for starters. For random example, it doesn&amp;rsquo;t shame a user that may have dropped out of high school by leaving education a permanent requirement—a user can now just choose not applicable. Here is a recent (as of 12/12/2017) design of LinkedIn, for a user just getting started:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/linkedin1.jpg&#34; alt=&#34;LinkedIn&#34; title=&#34;LinkedIn Pre-Removal&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While not coercive, it does gently nudge the user to add current location. Being so close to an award motivates my wife to drink as much water possible, and to get her exercise in. People will go a long way for that little reward, and if they are cognizant of this and don&amp;rsquo;t want to, they can just use FIXBUX to remove that urge.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/linkedin2.jpg&#34; alt=&#34;LinkedIn&#34; title=&#34;LinkedIn Post-Removal&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Done. The HTML is gone from your browser. The effects of this can be debated. It can be argued that location is an important parameter when job hunting and networking, but it can also be argued that any data we put online can be circulated and aggregated into something more meaningful about us and our lives. This design seems benign, but perhaps it isn&amp;rsquo;t, and it is worth giving users the flexibility to reduce their cognitive burden in dealing with coercive patterns, or in this case, rewards.&lt;/p&gt;

&lt;h3 id=&#34;handshake&#34;&gt;Handshake&lt;/h3&gt;

&lt;p&gt;Handshake is less gentle than LinkedIn, and is a prime example of coercive design. Below shows an example of some job experience:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/handshake.jpg&#34; alt=&#34;Handshake&#34; title=&#34;Pre-Removal&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There are a great number of details here, from position, to company, location, and experience. Further down, not shown, there are courses taken, projects worked on, social links that are entered, skills, and a short bio. Above, previous and current colleges are listed along with dates, major, degrees earned, class, GPA, and a picture. With all of this data submitted, that little orange progress bar makes me feel pretty unaccomplished, like I need to do and add more. If I removed any of the content in my page, the bar would turn red, and there would be a frowny face next to it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/bullyux.jpg&#34; alt=&#34;Handshake&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I could add more, but I don&amp;rsquo;t want to, and the information that is there already, isn&amp;rsquo;t really worth giving to a company like this. Cue FIXBUX. Anyone can remove these HTML elements without giving a second thought.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/handshake_removed.jpg&#34; alt=&#34;Handshake&#34; title=&#34;Handshake Post-Removal&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Better, right? Companies will always try to compel people into giving more data about themselves, and people will always behave like people. FIXBUX is a rational tool to use in such an environment.&lt;/p&gt;

&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;FIXBUX removes HTML elements with simple Javascript and jQuery. Removing the LinkedIn site&amp;rsquo;s progress bar was fairly straightforward since they use a simple naming convention. It can be done with this very basic function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Javascript&#34;&gt;//Remove LinkedIn Progress Bar
$(document).ready(function(){
    $(&#39;div&#39;).remove(&#39;.pv-pcm-progress&#39;);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Handshake, on the other hand, was a different beast. Being fairly inexperienced with Javascript, I did not know how to deal with the following format of the HTML element:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/weirdstyling.jpg&#34; alt=&#34;Handshake HTML&#34; title=&#34;Weird&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I tried variations of the last function, referencing things like style(underscore* underscore* progress-bar and progress-bar, but it was to no avail. With the help from others, I learned of new &lt;a href=&#34;https://api.jquery.com/attribute-starts-with-selector/&#34;&gt;selector syntax&lt;/a&gt;. Using the code&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Javascript&#34;&gt;console.log($(&amp;quot;[class^=&#39;style__progress&#39;]&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I could test different things, like the wildcard, to see if I got any match. The above code worked. At this point it was just adapting it the same way as my previous function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Javascript&#34;&gt;//Remove Handshake Progress Bar
$(document).ready(function() {
    $(&amp;quot;[class^=&#39;style__progress&#39;]&amp;quot;).remove()
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In essence, whenever the extension is enabled, once the page is loaded (all of its elements), the progress bar is removed from the page. In surprisingly little code, a user can be granted more freedom in what designs they are exposed to, which can then influence their behavior and how they share their data with other people and organizations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VMs and Usability Testing</title>
      <link>https://ckhoward.github.io/blog/vms-and-usability-testing/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/vms-and-usability-testing/</guid>
      <description>

&lt;h1 id=&#34;vms-for-usability-testing&#34;&gt;VMs for Usability Testing&lt;/h1&gt;

&lt;h2 id=&#34;dependency-nightmares&#34;&gt;Dependency Nightmares&lt;/h2&gt;

&lt;p&gt;This might be an unpopular opinion, but I imagine virtual machines could be a useful tool for usability testers. When it comes to learning a new software or using a new tool, it&amp;rsquo;s the absolute worst finding out that certain dependencies aren&amp;rsquo;t listed in the requirements, or that they are not consistent with your own. This was a pain I experienced when I first started learning Ruby on Rails and MongoDB, and the tutorials I found wouldn&amp;rsquo;t work. The experience was so painful, in fact, that as a young teen, I gave up on these tools. A way to avoid this problem when we create our own tutorials and products may be to use a VM or Container.&lt;/p&gt;

&lt;p&gt;VMs and Containers are a great way to beta-test whatever the team&amp;rsquo;s technical writers produced in terms of &lt;strong&gt;Getting Started&lt;/strong&gt; text, particularly for install steps. Many associate these tools with rigorous scientific work, but fundamentally, they force you to start using some piece of software from scratch.&lt;/p&gt;

&lt;h2 id=&#34;virtualizing-for-a-blank-canvas&#34;&gt;Virtualizing for a Blank Canvas&lt;/h2&gt;

&lt;p&gt;When we start building, our mind becomes a bit warped and our perspective changes as the product changes, and often this perspective is very different from that of a new user. For example, when working on a data pipeline for iNaturalist butterfly data to be processed through a species distribution model, our team went into the project with IPython, Miniconda, Jupyter, Ubuntu, and R all pre-installed on our work machines. When creating the tutorial, we knew Python 3.6, Bash, and R (and its dependencies) were all requirements, so they were listed under the install steps. Then, for the sake of reproducibility, we wanted to build a VM to make it easier for our clients to get their work done.&lt;/p&gt;

&lt;p&gt;We built the VM. Following the tutorial, we installed the requirements, and ran the application. No output. Not only did we realize that R (and its packages) had to be installed through Ubuntu, but we also found some fatal, though easily fixed, directory code. The code worked fine in the builds on our side, but it’s only because somehow, I hadn’t noticed that I created a couple of directories for testing that I hadn’t remembered, but referenced in code. My mind just shifted with the product. Lesson learned, using the VM was a great way of conducting usability testing and testing the soundness of our documentation and code.&lt;/p&gt;

&lt;p&gt;When building products for other people, we want them to have the best experience possible. It is trivially simple to throw together a Virtual Machine or Container to start where a new user may be starting. As nice as it is to be contacted by somebody using your product, it’s not so nice when they express frustration that it isn’t working, or that the tutorial is flawed because it was perfectly followed to no avail. Put yourself in their shoes and do some usability testing with a VM.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Epistemology of Testimony</title>
      <link>https://ckhoward.github.io/blog/the-epistemology-of-testimony/</link>
      <pubDate>Wed, 04 Oct 2017 17:01:31 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/the-epistemology-of-testimony/</guid>
      <description>

&lt;h1 id=&#34;the-epistemology-of-testimony&#34;&gt;The Epistemology of Testimony&lt;/h1&gt;

&lt;p&gt;As humans, our primary way of learning about the world is through experience. But given that we aren&amp;rsquo;t omnipresent and can&amp;rsquo;t experience everything for ourselves, we have to fall back on another method—teamwork. We create information and share it with others, and they do the same. Pretty much everything we learn about the world, apart from what we learn through observation and experience, is based on testimony, or what we hear and read. Naturally, the testimony of others is incredibly important to who we are and what we know.&lt;/p&gt;

&lt;p&gt;Assessing information to be true or false calls for a degree of consciousness. Hume makes an interesting point when he says:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We entertain a suspicion concerning any matter of fact, when the witnesses contradict each other; when they are but few, or of a doubtful character; when they have an interest in what they affirm; when they
deliver their testimony with hesitation, or on the contrary, with too violent asseverations. There are many other particulars of the same kind, which may diminish or destroy the force of any argument, derived from
human testimony.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Our judgments and perceptions shape how we think and feel about who somebody is and if we believe them; what that person is saying; how that person is saying it; and why. Maybe most academics treat this as a subjective probability, on a scale from 0 to 1. I think [-1,1] is a more useful interval, depicted as a spectrum, when thinking about how we interact with the testimony of others. Naturally, for some new event, somebody starts at 0. Depending on our judgments, we may be pushed towards disbelief (-1), or belief (1).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/gradient.jpg&#34; alt=&#34;img&#34; title=&#34;A scale of [-1, 1]&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To get at Hume&amp;rsquo;s points, we should talk about consciousness. Self-awareness is a central issue when it comes to the epistemology of testimony, because the way we deal with other people and information is complex, and often out of focus. More time should be given to understanding how we judge people and the information they convey to us, and how they convey it.&lt;/p&gt;

&lt;h2 id=&#34;judging-communication&#34;&gt;Judging Communication&lt;/h2&gt;

&lt;p&gt;As noted, communicating violently can destroy a person&amp;rsquo;s argument. It doesn&amp;rsquo;t matter if they are right or wrong, an expert or not, once the arguer (or conveyor) of information becomes aggressive, or condescending, or anything with a nasty connotation, the point is lost. Communication really is a key part of knowledge acquisition. It is unfortunate that people are notoriously bad communicators. We become overly defensive at times, have trouble letting others know how we actually feel, and we have a hard time understanding what others actually mean, even if they aren&amp;rsquo;t quite delivering their message as they should be.&lt;/p&gt;

&lt;h2 id=&#34;judging-character&#34;&gt;Judging Character&lt;/h2&gt;

&lt;p&gt;We have to determine a person&amp;rsquo;s authority or credibility so that we can attach some proportion of belief or disbelief to what they tell us. Sometimes language patterns can help us with this; something I have noticed is that conspiracy theorists often have distinctive styles of writing. When I see somebody intermittently throwing in a &amp;lsquo;TRUTH!!!,&amp;rsquo; or calling people &amp;lsquo;sheeple,&amp;rsquo; I automatically attach some (likely great) proportion of disbelief to whatever comes next. When looking at an email that links you to some sign-in form, but you see broken and fragmented Englesh, you may suspect that the sender is not credible, and that it isn&amp;rsquo;t safe to sign in. When it comes to judging other people&amp;rsquo;s anecdotes, I have found one of my personal biases. When I am taking in information, when a source admits it was wrong, but provides &amp;lsquo;evidence&amp;rsquo; immediately afterwards (for what brought his or her revelation), I&amp;rsquo;m much more likely to believe it, or at least my guard is let down. The revelatory aspect is appealing to me, it seems. I probably think an introspective quality in somebody, or that sort of intellectual flexibility to change viewpoint, is something that warrants belief or faith. Reflecting back on Judging Communication, emotion and demeanor can also be ways to judge character.&lt;/p&gt;

&lt;h2 id=&#34;judging-information&#34;&gt;Judging Information&lt;/h2&gt;

&lt;p&gt;Some years back, I stumbled upon a blog post called &lt;a href=&#34;https://youarenotsosmart.com/2011/06/10/the-backfire-effect/&#34;&gt;The Backfire Effect&lt;/a&gt;. It may not be the most scientific source, but I highly recommend reading it because A) it is supported by some sociological studies (for what that is worth) and B) the idea is relevant to communication, and extends to every Facebook argument you have ever seen. The gist of it is that, when presented with information contrary to our beliefs, we have an almost primal inclination to keep our beliefs safe from it. We naturally spend time confirming ideas, finding pieces here and there that are supportive of the idea, and ignoring pieces that are not. And when it comes down to it, this new, contradictory information can actually push us further into believing whatever it is the antagonist tries to counter. When we talk about knowledge acquisition and how it relates to what we are hearing or reading from somebody else, this is the sort of bias that can do a lot of damage in acquiring knowledge. I have seen a phrase pop up that goes &amp;ldquo;strong opinions, loosely held,&amp;rdquo; but I have a difficult time wrapping my head around it. If anyone has any thoughts about this phrase, feel free to share.&lt;/p&gt;

&lt;p&gt;When judging information, timeliness is also a relevant factor. We may see a high-reputation journalist hastily publish details incorrectly. After the break of a big story, where people are still scrambling to gather the facts, it can be useful take pause before consuming and propagating information, as it can have dangerous effects. When considering this Las Vegas shooting, the Gateway Pundit (a political site that some consider credible) &lt;a href=&#34;https://www.nytimes.com/2017/10/02/us/politics/viral-claims-and-rumors-in-the-las-vegas-shooting.html&#34;&gt;wrongly published somebody else being the gunman&lt;/a&gt;, resulting in many death threats being made to the victim and his family. And if view-count makes a YouTube video more credible, just look at this Tweet: &lt;a href=&#34;https://twitter.com/tqbf/status/915630300381745152&#34;&gt;https://twitter.com/tqbf/status/915630300381745152&lt;/a&gt; Given a little bit of time, people will delete their Tweets, edit their published article, or flag and delete videos like the one above.&lt;/p&gt;

&lt;h3 id=&#34;comments&#34;&gt;Comments&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;d love to know what all of you think about the epistemology of testimony. How do you approach particular comments; which ones seem wise, or ill-informed? What kinds of factors influence the believability of a piece of information for you? Do you have any policies that you use to protect yourself and others from mis- and dis-information? What kinds of responsibilities might be relevant to using testimony as grounds of gaining and sharing knowledge? Do you have any biases in particular that you&amp;rsquo;re interested in? Thanks for reading.&lt;/p&gt;

&lt;h3 id=&#34;sources&#34;&gt;Sources&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://ccl.northwestern.edu/papers/ABMVisualizationGuidelines/palette/scheme-color-scale-gradient.png&#34;&gt;http://ccl.northwestern.edu/papers/ABMVisualizationGuidelines/palette/scheme-color-scale-gradient.png&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Path Distance Analysis [GIS]</title>
      <link>https://ckhoward.github.io/blog/path-distance-analysis-gis/</link>
      <pubDate>Tue, 15 Aug 2017 19:25:23 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/path-distance-analysis-gis/</guid>
      <description>

&lt;h1 id=&#34;path-distance-analysis&#34;&gt;Path-Distance Analysis&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;//This is a fictitious scenario//&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s suppose we&amp;rsquo;re a team of highly skilled NGA agents. We just received a briefing about some suspicious activity between two rival factions. These factions are typically hostile towards each other, but a trusted informant has brought forward some intel suggesting the two factions have been cooperating.&lt;/p&gt;

&lt;p&gt;Our team must assist in verifying these claims. Wielding the all-powerful eye in the sky — or, an array of US Government satellites — we will first identify which outposts are linked to each faction, and we will then infer the inter- and intra-faction transportation networks. This will prove valuable to our client, giving them direction when choosing what to surveil so that they can confirm or deny the informant&amp;rsquo;s claims — that these two factions are indeed working together.&lt;/p&gt;

&lt;p&gt;The two factions are indicated by stars and all of the outposts are indicated with green points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd1.png&#34; alt=&#34;alt text&#34; title=&#34;Aerial shot of two factions within the city&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So we have this overview. By itself, it isn&amp;rsquo;t all that helpful. We need more data if we want to find out which factions own which outposts. Naturally, being the National Geospatial-Intelligence Agency, our team has some useful road-quality data associated with the area.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd2.png&#34; alt=&#34;alt text&#34; title=&#34;digital elevation model for area of interest&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd3.png&#34; alt=&#34;alt text&#34; title=&#34;slopes for area of interest&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;methods-in-path-distance-analysis&#34;&gt;Methods in Path-Distance Analysis&lt;/h2&gt;

&lt;p&gt;The objective is to find out which green points are associated with which stars. You can think of each star as a sort of command post. If we think about it, it&amp;rsquo;s most likely a matter of proximity. You don&amp;rsquo;t want your outposts in enemy territory. So we want to look at path-distance. But it isn&amp;rsquo;t so black and white.&lt;/p&gt;

&lt;h3 id=&#34;euclid-s-distance&#34;&gt;Euclid&amp;rsquo;s Distance&lt;/h3&gt;

&lt;p&gt;Euclidean distance is basically straight-line distance. Below, you can suppose that each ring is approximately a mile long.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd4.png&#34; alt=&#34;alt text&#34; title=&#34;Euclid&#39;s Linear Distance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And if this method were right, we could just find where the rings intersect, divide the area into two, and observe where the outposts fall. If the outposts are in the half containing the red faction, the red faction owns them. If the outposts are in the half containing the blue faction, the blue faction owns them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd5.png&#34; alt=&#34;alt text&#34; title=&#34;Euclid&#39;s Method in Determining Outpost-Faction Relationship&#34; /&gt;
&lt;img src=&#34;img/pd6.png&#34; alt=&#34;alt text&#34; title=&#34;Area being Split by Euclidean Terms&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, paths are more complex. Terrain, wind, road condition, stop-lights, and other elements can influence time to get from point A to point B; you simply can&amp;rsquo;t walk through a mountain. Lucky for us, there are other means of calculating the true distance of paths.&lt;/p&gt;

&lt;h3 id=&#34;arcgis-path-distance&#34;&gt;ArcGIS Path Distance&lt;/h3&gt;

&lt;p&gt;ArcGIS doesn&amp;rsquo;t suffer the pitfalls of the Euclidean method when calculating distance; it comes equipped with a very versatile Path Distance function. As noted, certain environmental factors can slow you down or speed you up on a given path. The ArcGIS function can account for these factors.&lt;/p&gt;

&lt;p&gt;The Path Distance function requires a cost surface, so we will build one. This cost surface is a raster that shows the level of effort required to traverse some area. For simplicity, we will use road quality to account for cost, but GIS software can do a ton more, mathematically incorporating a huge variety of other factors. Intuitively, red areas indicate awful, destroyed roads.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd7.png&#34; alt=&#34;alt text&#34; title=&#34;Road Quality in Cost Surface&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can then &lt;a href=&#34;http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/creating-a-cost-surface-raster.htm&#34;&gt;reclassify&lt;/a&gt; these data. Values are reclassified to more accurately portray cost. In this case, values rise exponentially to reflect how much more effort is used to traverse the different roads. Using a road under full construction isn&amp;rsquo;t possible. Reclassifying data accounts for the relativity of the situation at-hand, as driving a car down a bumpy road will have a different cost than walking down the bumpy road.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd23.png&#34; alt=&#34;alt text&#34; title=&#34;Reclassifying Cost Surface&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd8.png&#34; alt=&#34;alt text&#34; title=&#34;Reclassified Cost Surface&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;path-distance-and-backlink-rasters&#34;&gt;Path Distance and Backlink Rasters&lt;/h2&gt;

&lt;p&gt;Now we can use our Path Distance function. It uses our command posts (the stars) as input points and it uses our cost raster and surface raster to calculate the cost of navigating the area. It will create two new rasters. One is the path distance model and the other is the backlink raster.&lt;/p&gt;

&lt;h3 id=&#34;path-distance-raster&#34;&gt;Path Distance Raster&lt;/h3&gt;

&lt;p&gt;Every cell in this raster holds the least cost accumulated. To put this more simply, it says that we&amp;rsquo;re starting at a command post (either, it doesn&amp;rsquo;t matter which). But we want to leave, without having defined any place we&amp;rsquo;re going. So we start walking. Each step we take, we will have a number of directions we can go, some with higher cost than others (because of road-quality changes). If we think about this in terms of cells, we&amp;rsquo;re on some given cell, and the cells around it have associated cost values. This new raster shows the least cost path whichever way you want to go.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd9.png&#34; alt=&#34;alt text&#34; title=&#34;Area being Split by Euclidean Terms&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;backlink-raster&#34;&gt;Backlink Raster&lt;/h3&gt;

&lt;p&gt;The backlink raster is basically a way-finder, from any (and every) cell, back to the origin (our command posts). The graphic may be a complete mess, seeming entirely incoherent. It&amp;rsquo;s actually really cool though. Each color is associated with a number, and each number is associated with a direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd26.png&#34; alt=&#34;alt text&#34; title=&#34;Back Link Raster Directions from ArcGIS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The number 7 tells you to go north, which in our case is the color yellow. And you follow its directions because it is the least-cost path. In other words, your cell is yellow and telling you to go north because the other cells (NE, E, SE, S, SW, W, NW) all come at a higher cost. Think of it as a color-coded way of getting out of danger.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd21.png&#34; alt=&#34;alt text&#34; title=&#34;Back Link Raster&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The backlink raster will be used to calculate the true relationship between the two command posts and the dozens of outposts. The Cost Path tool takes the smaller outposts as starting points, then uses the instructions of the backlink to make their way back to the command posts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd20.png&#34; alt=&#34;alt text&#34; title=&#34;Cost Path with Backlink&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;Or to make it a little more clear:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd11.png&#34; alt=&#34;alt text&#34; title=&#34;Cost Path without Backlink&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Awesome. So we can finally see the relationships. In this case, it just so happens that Euclid&amp;rsquo;s method was pretty close, it just missed almost all of the points that were immediately on the other side of the division line. Though it took a little extra calculation, factoring in other variables like road-quality turned out to be worth it.&lt;/p&gt;

&lt;h2 id=&#34;inferring-transportation-routes-with-corridor-analysis&#34;&gt;Inferring Transportation Routes with Corridor Analysis&lt;/h2&gt;

&lt;p&gt;Corridor analysis finds the optimal corridor between two points. This is essentially finding the optimal areas of transportation, as opposed to one explicit path. The Corridor tool requires two path distance rasters. Each raster will have least cost paths surrounding some source, in our case, a command post.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd27.png&#34; alt=&#34;alt text&#34; title=&#34;Path Distance for Each Source&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The two rasters will then be summed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd14.png&#34; alt=&#34;alt text&#34; title=&#34;Summed Path Distances&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This corridor is hardly insightful. There is too much area between the two factions where transportation is low-cost. We can create thresholds to check for smaller values, so that we can narrow down the number of potential routes. This is done with conditional statements. We&amp;rsquo;re trying to highlight lower-value areas and according to the key, the range of data is [7,690-168,354]. These data vary between projects so trial-and-error is really the best bet for finding a threshold we like. Using Raster Calculator, we will first try finding data that is less than 10,000 with our conditional statement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Con(&amp;ldquo;corridor_surface&amp;rdquo;&amp;lt;10000,1)&lt;/strong&gt; — For each cell, if cell value is less than 10,000, write 1 to cell in output raster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd15.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Threshold 10000&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Con(&amp;ldquo;corridor_surface&amp;rdquo;&amp;lt;9000,1)&lt;/strong&gt; — For each cell, if cell value is less than 9,000, write 1 to cell in output raster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd16.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Threshold 9000&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Pretty good, but we can narrow it down just a little more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Con(&amp;ldquo;corridor_surface&amp;rdquo;&amp;lt;8400,1)&lt;/strong&gt; — For each cell, if cell value is less than 8,400, write 1 to cell in output raster.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd17.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Threshold 8400&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is good enough. If we narrow it down too far, we would be unreasonably assuming that these factions use the most optimal route possible to travel between command posts. This isn&amp;rsquo;t realistic. Fortunately, this threshold served its purpose in narrowing down possible routes enough to be insightful. There are three major bottlenecks that could be strategically used to switch out vehicles when tailing a target.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd29.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Overlaid on City&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/pd30.png&#34; alt=&#34;alt text&#34; title=&#34;Corridor Overlaid on City&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With our GIS, we have now inferred the relationship between the two factions and the marked outposts. We also created a great deal of actionable intelligence regarding factions&amp;rsquo; transportation networks. From this, we can consider which areas might be at-risk to violence; which outposts are weaker due to having a low degree of centrality within its faction&amp;rsquo;s network; which outposts are likely to be hotspots; and which areas a surveillence team might work most effectively in.&lt;/p&gt;

&lt;h3 id=&#34;data-source&#34;&gt;Data Source:&lt;/h3&gt;

&lt;p&gt;The University of Arizona&lt;/p&gt;

&lt;p&gt;Advanced GIS (GIST 420)&lt;/p&gt;

&lt;p&gt;Professor Gary Christopherson&lt;/p&gt;

&lt;p&gt;The data used comes from class exercises not related to this scenario.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Analysis of First GOP Debate in 2015 [R]</title>
      <link>https://ckhoward.github.io/blog/sentiment-analysis-of-first-gop-debate-in-2015-r/</link>
      <pubDate>Sat, 17 Jun 2017 14:50:42 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/sentiment-analysis-of-first-gop-debate-in-2015-r/</guid>
      <description>

&lt;h1 id=&#34;a-statistical-analysis-of-sentiments&#34;&gt;A Statistical Analysis of Sentiments&lt;/h1&gt;

&lt;p&gt;Every four years, the United States goes through the process of electing (or re-electing) its president. Politics becomes a popular topic of conversation, and inadvertently, a popular emotional outlet. Our digital landscape has — mostly textually, but sometimes by video or podcast — granted the ability for people to express their thoughts and feelings on political ideas and events, en masse. Suffice to say, an examination of people&amp;rsquo;s language in these expressions can yield many useful insights into human (or &lt;em&gt;American&lt;/em&gt;) character and the influence of rhetoric on political philosophy and national pride. Therefore, to touch on this examination of language, I will be exploring the sentiments (and their respective confidences), of Tweets pertaining to the first GOP debate, that was hosted on August 6th, 2015.&lt;/p&gt;

&lt;h2 id=&#34;the-twitter-sentiment-dataset&#34;&gt;The Twitter-Sentiment Dataset&lt;/h2&gt;

&lt;p&gt;The dataset being explored links Tweets to relevant data such as: issues (e.g. abortion), candidates being responded to (and respective confidence), retweet count, and other metadata, but most importantly, sentiments and their respective confidences. We can load our data into an R dataframe with the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sentiment &amp;lt;- read.csv(&amp;quot;~path/Sentiment.csv&amp;quot;)
View(Sentiment)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot8.png&#34; alt=&#34;The Sentiment Table&#34; title=&#34;The Sentiment.csv Table&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The column labeled &lt;em&gt;sentiment&lt;/em&gt; is what we&amp;rsquo;re interested in so we create two frames, one for negative sentiments, and one for positive. We aren&amp;rsquo;t worried about neutral.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Indexes the sentiment confidence for values associated with positive sentiments
(pos &amp;lt;- Sentiment$sentiment_confidence[Sentiment$sentiment == &amp;quot;Positive&amp;quot;])

#Indexes the sentiment confidence for values associated with negative sentiments
(neg &amp;lt;- Sentiment$sentiment_confidence[Sentiment$sentiment == &amp;quot;Negative&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;confidence-hypotheses&#34;&gt;Confidence Hypotheses&lt;/h2&gt;

&lt;p&gt;My alternative hypothesis is that the mean confidence of negative sentiments will be greater than the mean confidence of positive sentiments; thus, my null hypothesis is that the mean confidence of negative sentiments will be lower than, or equal to, the mean confidence of positive sentiments. The alternative hypothesis is one-tailed.&lt;/p&gt;

&lt;h2 id=&#34;descriptive-statistics-of-sentiment-confidences&#34;&gt;Descriptive Statistics of Sentiment Confidences&lt;/h2&gt;

&lt;p&gt;For this exploration, the mean is chosen as the measure of central tendency because the data is numerical and does not contain outliers. We can find and plot the means of the negative and positive frames quickly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Creates a variable for the mean of the indexed positive sentiment confidence levels
(mean_pos = mean(pos))
#[1] 0.7144841

#Creates a variable for the mean of the indexed negative sentiment confidence levels
(mean_neg = mean(neg))
#[1] 0.8003269

#Creates barplot with a y-scale of [0,1]
barplot(c(mean_pos, mean_neg), names.arg = c(&amp;quot;Positive Sentiments&amp;quot;, &amp;quot;Negative Sentiments&amp;quot;), xlab = &amp;quot;Mean Confidences of Positive and Negative Sentiments&amp;quot;, width = .5, xlim = c(0, 2), ylim = c(0, 1), space = .6, col = c(&amp;quot;darkolivegreen3&amp;quot;, &amp;quot;darkorchid3&amp;quot;), main = &amp;quot;Mean Confidence per Sentiment&amp;quot;, ylab = &amp;quot;Mean Confidence Level&amp;quot;)

#Creates a dotplot with diamond symbol for better visibility
dotchart(c(mean_neg, mean_pos), labels = c(&amp;quot;Negative Sentiments&amp;quot;, &amp;quot;Positive Sentiments&amp;quot;), main = &amp;quot;Mean Confidence per Sentiment&amp;quot;, xlab = &amp;quot;Mean Confidence Level&amp;quot;, color = c(&amp;quot;darkorchid3&amp;quot;, &amp;quot;darkolivegreen3&amp;quot;), pch = 9)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot1.png&#34; alt=&#34;Mean Barplots&#34; title=&#34;Barplot of Positive and Negative Mean Confidences&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/plot2.png&#34; alt=&#34;Mean Dotplots&#34; title=&#34;Dotplot of Positive and Negative Mean Confidences&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Above, there are two representations comparing the mean confidence levels, per positive and negative sentiment, via the sample provided in the dataset, containing 13,871 observations. The first figure shows the different levels of confidence within the scope of the entire scale (0 to 1). It is evident that the mean confidence for negative sentiments is greater than the mean confidence of positive.&lt;/p&gt;

&lt;p&gt;The second figure compares the levels of confidence within the approximate scope of the values themselves (~.70 to ~.81). When calculated, the positive mean confidence is 0.714484 while the negative mean confidence is .800327. Relative to the entire population this is not a large number of observations. Therefore, for a better representation of that population (all Tweets regarding the debate), I will bootstrap resample the observation 10,000 times.&lt;/p&gt;

&lt;h2 id=&#34;inferential-statistics&#34;&gt;Inferential Statistics&lt;/h2&gt;

&lt;p&gt;Bootstrapping will also be done with R code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Resamples pos vector, takes the mean, then replicates 10,000 times for 10,000 means, stores in variable
(resamples_pos = replicate(10000, mean(sample(pos, replace = T))))

#Resamples neg vector, takes the mean, then replicates 10,000 times for 10,000 means, stores in variable
(resamples_neg = replicate(10000, mean(sample(neg, replace = T))))

#Results in distribution of the difference in negative mean distribution and positive mean distribution, stores in variable
(difference_distro = resamples_neg - resamples_pos) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After bootstrap resampling the data 10,000 times, the calculated means are: Mean(positive) = 0.714436 and Mean(negative) = .800329, both staying relatively the same, with Mean(negative - positive) = .0859. Further, the difference in these distributions can be visualized.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Expands margins to fit the entirety of the legend
par(xpd=NA)

#Creates histogram of positive and negative mean distributions for easy comparison
#The add=T argument allows us to add multiple graphs in one plot
positive_distro = hist(resamples_pos, freq = T, col = &amp;quot;darkolivegreen3&amp;quot;, xlim = c(.69, .815), ylim = c(0, 1800), main = &amp;quot;Bootstrapped Mean Confidences per Sentiment (10,000 Resamples per)&amp;quot;, xlab = &amp;quot;Confidence Level&amp;quot;)

#As noted above, add = T adds this histogram distribution to the above histogram
negative_distro = hist(resamples_neg, freq = T, col = &amp;quot;darkorchid3&amp;quot;, add = T)

#Creates a legend for the histogram(s) above; inset contributes to position
legend(&amp;quot;topright&amp;quot;, inset=c(-0.02,0), c(&amp;quot;Positive&amp;quot;, &amp;quot;Negative&amp;quot;), col = c(&amp;quot;darkolivegreen3&amp;quot;, &amp;quot;darkorchid3&amp;quot;), lwd = 5)

#Produces vertical lines for the negative sentiment confidence mean, then for positive
abline(v = mean(resamples_neg), lwd = 2)
abline(v = mean(resamples_pos), lwd = 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot6.png&#34; alt=&#34;Bootstrap Resampling&#34; title=&#34;Bootstrapped Mean Confidences per Sentiment (10,000 Resamples&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Above, we can see the distributions of resampled means in confidence for both positive and negative sentiments, with the black vertical lines indicating the null hypothesis parameter values. This clearly shows that the mean confidence level is greater for the negative sentiment than it is for the positive. This is useful to visualize, but it might be nice to calculate and plot the actual difference in the distributions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Produces distribution of difference between resampled confidences per sentiment
hist(difference_distro, col = &amp;quot;darkslategray3&amp;quot;, freq = F, main = &amp;quot;Difference of Mean Confidence Levels between Sentiments&amp;quot;, xlab = &amp;quot;Confidence Level&amp;quot;, ylab = &amp;quot;Density&amp;quot;)

#Defines the C.I. being used for the hypothesis test
#This is also used for the difference distribution
(CI = quantile(difference_distro, c(0.05, 1))) 
#        5%       100% 
#0.07684648 0.10512917

#Vertical line that shows the C.I. frame 
#This is used to determine if null is rejected or not
abline(v = CI, lwd = 2)

#Further verification that neg - pos is not neg or 0, so null is rejected
(mean_diff = mean(resamples_neg) - mean(resamples_pos))
#[1] 0.08579504
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot7.png&#34; alt=&#34;Difference of Mean Confidences&#34; title=&#34;Difference of Mean Confidence Levels between Sentiments&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This last figure shows the distributed difference between the mean confidences in negative and positive sentiments. As the hypothesis is being tested with a 95% confidence interval, vertical markers have been placed at the 5% mark (at the value of .0769) and at the 100% mark (at the value of .1103).&lt;/p&gt;

&lt;h1 id=&#34;discussion&#34;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;When assessing the null (neg. mean sent. &amp;lt;= pos. mean sent.), we can conclude that the means are not equal as 0 does not lie within the C.I. frame, and we can conclude that the negative mean confidence is not less than the positive, as there are no negative numbers within the C.I. frame. Therefore, the null hypothesis is rejected and the alternative hypothesis is supported. We can say with 95% confidence that the mean confidence of negative sentiments is greater than the mean confidence of positive sentiments. In other words, in 95 of 100 samples, the difference between the mean confidence of negative tweets and the mean confidence of positive tweets will fall within the given interval.&lt;/p&gt;

&lt;p&gt;This might suggest that Twitter could be used, at least in a political context, to vent negative emotions, whether pertinent to politics or not. It could suggest something about these Twitter users in general; perhaps they are mostly liberals, who are more likely to disagree with GOP ideology and be more active online (in social media sites in particular, especially with younger people tending to be more liberal than conservative). Maybe this suggests something about the implicit and explicit nature of positive and negative language, that it&amp;rsquo;s easier to use and identify negative words than it is to use and identify positive words. Maybe candidates tend to appeal to strong emotions (typically negative) in order to appeal to potential voters. Regardless, there are more variables in this dataset that should be considered. Perhaps responses to Trump, or abortions, greatly skew the sentiments. It may be worth breaking the data down further to account for these variables (presidential candidates and issues). To come to a more significant conclusion, it is necessary to delve further into this data, data like it (other GOP debates, democratic debates, and independent debates), and data relevant to these other considerations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; A potential issue with this analysis is the algorithm that was used to assess whether a sentiment is neutral, negative, or positive; human intuition can do a much better job in accounting for context, sarcasm, and other subtleties of human language than language-processing algorithms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Source:&lt;/strong&gt; &lt;a href=&#34;https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment&#34;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Analysis of First GOP Debate in 2015 [R]</title>
      <link>https://ckhoward.github.io/post/gop-debate-sentiment-analysis/</link>
      <pubDate>Sat, 17 Jun 2017 14:50:42 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/post/gop-debate-sentiment-analysis/</guid>
      <description>

&lt;h1 id=&#34;a-statistical-analysis-of-sentiments&#34;&gt;A Statistical Analysis of Sentiments&lt;/h1&gt;

&lt;p&gt;Every four years, the United States goes through the process of electing (or re-electing) its president. Politics becomes a popular topic of conversation, and inadvertently, a popular emotional outlet. Our digital landscape has — mostly textually, but sometimes by video or podcast — granted the ability for people to express their thoughts and feelings on political ideas and events, en masse. Suffice to say, an examination of people&amp;rsquo;s language in these expressions can yield many useful insights into human (or &lt;em&gt;American&lt;/em&gt;) character and the influence of rhetoric on political philosophy and national pride. Therefore, to touch on this examination of language, I will be exploring the sentiments (and their respective confidences), of Tweets pertaining to the first GOP debate, that was hosted on August 6th, 2015.&lt;/p&gt;

&lt;h2 id=&#34;the-twitter-sentiment-dataset&#34;&gt;The Twitter-Sentiment Dataset&lt;/h2&gt;

&lt;p&gt;The dataset being explored links Tweets to relevant data such as: issues (e.g. abortion), candidates being responded to (and respective confidence), retweet count, and other metadata, but most importantly, sentiments and their respective confidences. We can load our data into an R dataframe with the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sentiment &amp;lt;- read.csv(&amp;quot;~path/Sentiment.csv&amp;quot;)
View(Sentiment)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot8.png&#34; alt=&#34;The Sentiment Table&#34; title=&#34;The Sentiment.csv Table&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The column labeled &lt;em&gt;sentiment&lt;/em&gt; is what we&amp;rsquo;re interested in so we create two frames, one for negative sentiments, and one for positive. We aren&amp;rsquo;t worried about neutral.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Indexes the sentiment confidence for values associated with positive sentiments
(pos &amp;lt;- Sentiment$sentiment_confidence[Sentiment$sentiment == &amp;quot;Positive&amp;quot;])

#Indexes the sentiment confidence for values associated with negative sentiments
(neg &amp;lt;- Sentiment$sentiment_confidence[Sentiment$sentiment == &amp;quot;Negative&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;confidence-hypotheses&#34;&gt;Confidence Hypotheses&lt;/h2&gt;

&lt;p&gt;My alternative hypothesis is that the mean confidence of negative sentiments will be greater than the mean confidence of positive sentiments; thus, my null hypothesis is that the mean confidence of negative sentiments will be lower than, or equal to, the mean confidence of positive sentiments. The alternative hypothesis is one-tailed.&lt;/p&gt;

&lt;h2 id=&#34;descriptive-statistics-of-sentiment-confidences&#34;&gt;Descriptive Statistics of Sentiment Confidences&lt;/h2&gt;

&lt;p&gt;For this exploration, the mean is chosen as the measure of central tendency because the data is numerical and does not contain outliers. We can find and plot the means of the negative and positive frames quickly.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Creates a variable for the mean of the indexed positive sentiment confidence levels
(mean_pos = mean(pos))
#[1] 0.7144841

#Creates a variable for the mean of the indexed negative sentiment confidence levels
(mean_neg = mean(neg))
#[1] 0.8003269

#Creates barplot with a y-scale of [0,1]
barplot(c(mean_pos, mean_neg), names.arg = c(&amp;quot;Positive Sentiments&amp;quot;, &amp;quot;Negative Sentiments&amp;quot;), xlab = &amp;quot;Mean Confidences of Positive and Negative Sentiments&amp;quot;, width = .5, xlim = c(0, 2), ylim = c(0, 1), space = .6, col = c(&amp;quot;darkolivegreen3&amp;quot;, &amp;quot;darkorchid3&amp;quot;), main = &amp;quot;Mean Confidence per Sentiment&amp;quot;, ylab = &amp;quot;Mean Confidence Level&amp;quot;)

#Creates a dotplot with diamond symbol for better visibility
dotchart(c(mean_neg, mean_pos), labels = c(&amp;quot;Negative Sentiments&amp;quot;, &amp;quot;Positive Sentiments&amp;quot;), main = &amp;quot;Mean Confidence per Sentiment&amp;quot;, xlab = &amp;quot;Mean Confidence Level&amp;quot;, color = c(&amp;quot;darkorchid3&amp;quot;, &amp;quot;darkolivegreen3&amp;quot;), pch = 9)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot1.png&#34; alt=&#34;Mean Barplots&#34; title=&#34;Barplot of Positive and Negative Mean Confidences&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/plot2.png&#34; alt=&#34;Mean Dotplots&#34; title=&#34;Dotplot of Positive and Negative Mean Confidences&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Above, there are two representations comparing the mean confidence levels, per positive and negative sentiment, via the sample provided in the dataset, containing 13,871 observations. The first figure shows the different levels of confidence within the scope of the entire scale (0 to 1). It is evident that the mean confidence for negative sentiments is greater than the mean confidence of positive.&lt;/p&gt;

&lt;p&gt;The second figure compares the levels of confidence within the approximate scope of the values themselves (~.70 to ~.81). When calculated, the positive mean confidence is 0.714484 while the negative mean confidence is .800327. Relative to the entire population this is not a large number of observations. Therefore, for a better representation of that population (all Tweets regarding the debate), I will bootstrap resample the observation 10,000 times.&lt;/p&gt;

&lt;h2 id=&#34;inferential-statistics&#34;&gt;Inferential Statistics&lt;/h2&gt;

&lt;p&gt;Bootstrapping will also be done with R code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Resamples pos vector, takes the mean, then replicates 10,000 times for 10,000 means, stores in variable
(resamples_pos = replicate(10000, mean(sample(pos, replace = T))))

#Resamples neg vector, takes the mean, then replicates 10,000 times for 10,000 means, stores in variable
(resamples_neg = replicate(10000, mean(sample(neg, replace = T))))

#Results in distribution of the difference in negative mean distribution and positive mean distribution, stores in variable
(difference_distro = resamples_neg - resamples_pos) 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After bootstrap resampling the data 10,000 times, the calculated means are: Mean(positive) = 0.714436 and Mean(negative) = .800329, both staying relatively the same, with Mean(negative - positive) = .0859. Further, the difference in these distributions can be visualized.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Expands margins to fit the entirety of the legend
par(xpd=NA)

#Creates histogram of positive and negative mean distributions for easy comparison
#The add=T argument allows us to add multiple graphs in one plot
positive_distro = hist(resamples_pos, freq = T, col = &amp;quot;darkolivegreen3&amp;quot;, xlim = c(.69, .815), ylim = c(0, 1800), main = &amp;quot;Bootstrapped Mean Confidences per Sentiment (10,000 Resamples per)&amp;quot;, xlab = &amp;quot;Confidence Level&amp;quot;)

#As noted above, add = T adds this histogram distribution to the above histogram
negative_distro = hist(resamples_neg, freq = T, col = &amp;quot;darkorchid3&amp;quot;, add = T)

#Creates a legend for the histogram(s) above; inset contributes to position
legend(&amp;quot;topright&amp;quot;, inset=c(-0.02,0), c(&amp;quot;Positive&amp;quot;, &amp;quot;Negative&amp;quot;), col = c(&amp;quot;darkolivegreen3&amp;quot;, &amp;quot;darkorchid3&amp;quot;), lwd = 5)

#Produces vertical lines for the negative sentiment confidence mean, then for positive
abline(v = mean(resamples_neg), lwd = 2)
abline(v = mean(resamples_pos), lwd = 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot6.png&#34; alt=&#34;Bootstrap Resampling&#34; title=&#34;Bootstrapped Mean Confidences per Sentiment (10,000 Resamples&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Above, we can see the distributions of resampled means in confidence for both positive and negative sentiments, with the black vertical lines indicating the null hypothesis parameter values. This clearly shows that the mean confidence level is greater for the negative sentiment than it is for the positive. This is useful to visualize, but it might be nice to calculate and plot the actual difference in the distributions.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Produces distribution of difference between resampled confidences per sentiment
hist(difference_distro, col = &amp;quot;darkslategray3&amp;quot;, freq = F, main = &amp;quot;Difference of Mean Confidence Levels between Sentiments&amp;quot;, xlab = &amp;quot;Confidence Level&amp;quot;, ylab = &amp;quot;Density&amp;quot;)

#Defines the C.I. being used for the hypothesis test
#This is also used for the difference distribution
(CI = quantile(difference_distro, c(0.05, 1))) 
#        5%       100% 
#0.07684648 0.10512917

#Vertical line that shows the C.I. frame 
#This is used to determine if null is rejected or not
abline(v = CI, lwd = 2)

#Further verification that neg - pos is not neg or 0, so null is rejected
(mean_diff = mean(resamples_neg) - mean(resamples_pos))
#[1] 0.08579504
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;img/plot7.png&#34; alt=&#34;Difference of Mean Confidences&#34; title=&#34;Difference of Mean Confidence Levels between Sentiments&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This last figure shows the distributed difference between the mean confidences in negative and positive sentiments. As the hypothesis is being tested with a 95% confidence interval, vertical markers have been placed at the 5% mark (at the value of .0769) and at the 100% mark (at the value of .1103).&lt;/p&gt;

&lt;h1 id=&#34;discussion&#34;&gt;Discussion&lt;/h1&gt;

&lt;p&gt;When assessing the null (neg. mean sent. &amp;lt;= pos. mean sent.), we can conclude that the means are not equal as 0 does not lie within the C.I. frame, and we can conclude that the negative mean confidence is not less than the positive, as there are no negative numbers within the C.I. frame. Therefore, the null hypothesis is rejected and the alternative hypothesis is supported. We can say with 95% confidence that the mean confidence of negative sentiments is greater than the mean confidence of positive sentiments. In other words, in 95 of 100 samples, the difference between the mean confidence of negative tweets and the mean confidence of positive tweets will fall within the given interval.&lt;/p&gt;

&lt;p&gt;This might suggest that Twitter could be used, at least in a political context, to vent negative emotions, whether pertinent to politics or not. It could suggest something about these Twitter users in general; perhaps they are mostly liberals, who are more likely to disagree with GOP ideology and be more active online (in social media sites in particular, especially with younger people tending to be more liberal than conservative). Maybe this suggests something about the implicit and explicit nature of positive and negative language, that it&amp;rsquo;s easier to use and identify negative words than it is to use and identify positive words. Maybe candidates tend to appeal to strong emotions (typically negative) in order to appeal to potential voters. Regardless, there are more variables in this dataset that should be considered. Perhaps responses to Trump, or abortions, greatly skew the sentiments. It may be worth breaking the data down further to account for these variables (presidential candidates and issues). To come to a more significant conclusion, it is necessary to delve further into this data, data like it (other GOP debates, democratic debates, and independent debates), and data relevant to these other considerations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; A potential issue with this analysis is the algorithm that was used to assess whether a sentiment is neutral, negative, or positive; human intuition can do a much better job in accounting for context, sarcasm, and other subtleties of human language than language-processing algorithms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Source:&lt;/strong&gt; &lt;a href=&#34;https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment&#34;&gt;Kaggle&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Little Bit of Monica in My Life</title>
      <link>https://ckhoward.github.io/blog/a-little-bit-of-monica-in-my-life/</link>
      <pubDate>Tue, 13 Jun 2017 17:01:31 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/a-little-bit-of-monica-in-my-life/</guid>
      <description>

&lt;h1 id=&#34;monica-the-personal-relationship-manager&#34;&gt;Monica, The Personal Relationship Manager&lt;/h1&gt;

&lt;p&gt;For those new to &lt;a href=&#34;https://monicahq.com/&#34;&gt;Monica&lt;/a&gt;, it&amp;rsquo;s a website that allows you to manage your personal relationships. You create contacts and provide information to make sure your encounters are meaningful and frequent (if that&amp;rsquo;s something you care about). You can add information about the contact&amp;rsquo;s significant other, their kids, where they might work, gifts, debts, social media accounts, birthdays, reminders, and even diets. In our busy minds, it can be hard to keep track of so many relevant details, and being human, we sometimes fall short of having &lt;em&gt;great&lt;/em&gt; personal interactions with others. A friend&amp;rsquo;s birthday might be forgotten, we might not check in with our mom as frequently as we&amp;rsquo;d like, and our conversations might not indicate that we truly care about what is going on in a friend&amp;rsquo;s life if we&amp;rsquo;ve forgotten to follow up on the latest major events in their life. These mishaps might not be overtly bad, but they can prevent us from having truly connected relationships. Who wouldn&amp;rsquo;t want better relationships?&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s better, is that Monica open-sources its code and emphasizes privacy. When it&amp;rsquo;s suggested that you aren&amp;rsquo;t being tracked and that your data isn&amp;rsquo;t being sold — a significant promise in an age where everybody is out for your data — it is verifiable.&lt;/p&gt;

&lt;p&gt;I was initially pulled toward the platform, but two pieces of anecdata really spoke to me and cemented my feelings enough to try it out:&lt;/p&gt;

&lt;p&gt;1) &lt;em&gt;danielvf&lt;/em&gt; from Hacker News &lt;a href=&#34;https://news.ycombinator.com/item?id=14498590&#34;&gt;writes&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My uncle died suddenly this year. He was unbelievably caring - and not just to family -
but to everyone he ever met. His funeral was jam packed with everyone
from homeless people to executives of multi-billion dollar companies.&lt;/p&gt;

&lt;p&gt;I always thought that his ability to always have you, and whatever you had
last talked about with him, on his mind at any moment was some kind of
supernatural gift. I was surprised to find out at his funeral that he
actually kept an excel spreadsheet of everyone he met and what they
needed and were going through. He reviewed this constantly.&lt;/p&gt;

&lt;p&gt;It didn&amp;rsquo;t lessen his genuine love for everyone, just let him be a little more super human.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2) In the book &lt;strong&gt;&lt;a href=&#34;https://www.amazon.com/How-Win-Friends-Influence-People/dp/0671027034&#34;&gt;How to Win Friends and Influence People&lt;/a&gt;&lt;/strong&gt;, a tip for getting the most out of the book is suggested:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The president of an important Wall Street bank once described, in
a talk before one of my classes, a highly efficient system he used for
self-improvement. This man had little formal schooling; yet he had
become one of the most important financiers in America, and he
confessed that he owed most of his success to the constant
application of his homemade system. This is what he does, I&amp;rsquo;ll put it
in his own words as accurately as I can remember.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;For years I have kept an engagement book showing all the
appointments I had during the day. My family never made any plans
for me on Saturday night, for the family knew that I devoted a part
of each Saturday evening to the illuminating process of self-examination
and review and appraisal. After dinner I went off by
myself, opened my engagement book, and thought over all the
interviews, discussions and meetings that had taken place during the
week. I asked myself:&lt;/p&gt;

&lt;p&gt;&amp;lsquo;What mistakes did I make that time?&amp;rsquo; &amp;lsquo;What did I do that was right and
in what way could I have improved my performance?&amp;rsquo; &amp;lsquo;What
lessons can I learn from that experience?&amp;rsquo;
&amp;ldquo;I often found that this weekly review made me very unhappy. I was
frequently astonished at my own blunders. Of course, as the years
passed, these blunders became less frequent. Sometimes I was
inclined to pat myself on the back a little after one of these sessions.
This system of self-analysis, self-education, continued year after
year, did more for me than any other one thing I have ever
attempted.
&amp;ldquo;It helped me improve my ability to make decisions - and it aided me
enormously in all my contacts with people. I cannot recommend it
too highly.&amp;rdquo;
Why not use a similar system to check up on your application of the
principles discussed in this book? If you do, two things will result.
First, you will find yourself engaged in an educational process that is
both intriguing and priceless.
Second, you will find that your ability to meet and deal with people
will grow enormously.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think I could use Monica as a tool for holding myself accountable when engaging with others, in a consistent, meaningful way. Monica could also facilitate the sort of introspection noted above, with its journal, activity, and reminder features. But while this is hugely appealing, I have some concerns. What if there was a breach?&lt;/p&gt;

&lt;h1 id=&#34;monica-in-the-crosshairs&#34;&gt;Monica in the Crosshairs&lt;/h1&gt;

&lt;p&gt;A breach would be very dangerous. Arguably Monica stores more information (not data) than Facebook. Where a picture on Facebook might represent an event, its participants, and its location, this is described in plaintext on Monica, making it easier to draw insights. A computer can parse this text much more easily than images. Additionally, since Monica is a privacy-oriented social manager, there is more of a &lt;em&gt;feeling&lt;/em&gt; of personal security, which could likely result in far more personal disclosure, or in other words, the willful release of more intimate details. In some sense, this makes users more vulnerable, personally, if there ever is such a breach. For those that want to draw parallels to encrypted messaging apps, I think the average person is going to write the same content in an SMS as they will a Signal message, and even if not, as of right now these encrypted-messaging apps are far more secure, so the targeting is less of a concern. I will, however, draw parallels to Facebook, because even though Monica isn&amp;rsquo;t a social network, it is in many ways comparable, and can serve many of Facebook&amp;rsquo;s primary functions.&lt;/p&gt;

&lt;p&gt;Notes of private conversations by definition aren&amp;rsquo;t shared on Facebook, but could be on Monica. Details of a relative or friend in a hospital could be implicitly and explicitly exploited. Drug addiction? Financial or spousal trouble? These are all negatives that could be taken advantage of, or could form the basis of a spear-phishing campaign. Even general data like a user&amp;rsquo;s workplace is often faked on Facebook, but on a platform like Monica, people will probably be more inclined to submit honest data.&lt;/p&gt;

&lt;p&gt;Many people first joined Facebook because they wanted to keep track of friends and family in the way Monica does. Facebook largely shifted, became less trustworthy, and became more superficial in users&amp;rsquo; eyes. A social manager that is open-sourced and private (where people are actually themselves and don&amp;rsquo;t self-censor), is an awesome alternative to something like Facebook. Unfortunately, the platform still has an http version that is usable, and the platform still lacks 2FA. These issues will likely be remedied soon, as security is the developer&amp;rsquo;s priority (along with data exporting). Less-savvy users will have to wait for these remedies. Technically proficient users, however, have more options due to Monica being open-source. The growing community has already developed installers for the platform (on Heroku and Docker); users can have much more power over their data by installing Monica on a server they manage. Though this is a good start, it is still worth exploring these considerations before committing large amounts of personal information to a web platform that may be extra prone to being targeted. I&amp;rsquo;m sure more is to come as the platform scales, and I would still easily choose Monica over other platforms any day. I just think users should be at least a little conscious of what kind of data they&amp;rsquo;re adding to their page, without assuming it will be perfectly secure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Little Bit of Monica in My Life</title>
      <link>https://ckhoward.github.io/post/a-little-bit-of-monica-in-my-life/</link>
      <pubDate>Tue, 13 Jun 2017 17:01:31 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/post/a-little-bit-of-monica-in-my-life/</guid>
      <description>

&lt;h1 id=&#34;monica-the-personal-relationship-manager&#34;&gt;Monica, The Personal Relationship Manager&lt;/h1&gt;

&lt;p&gt;For those new to &lt;a href=&#34;https://monicahq.com/&#34;&gt;Monica&lt;/a&gt;, it&amp;rsquo;s a website that allows you to manage your personal relationships. You create contacts and provide information to make sure your encounters are meaningful and frequent (if that&amp;rsquo;s something you care about). You can add information about the contact&amp;rsquo;s significant other, their kids, where they might work, gifts, debts, social media accounts, birthdays, reminders, and even diets. In our busy minds, it can be hard to keep track of so many relevant details, and being human, we sometimes fall short of having &lt;em&gt;great&lt;/em&gt; personal interactions with others. A friend&amp;rsquo;s birthday might be forgotten, we might not check in with our mom as frequently as we&amp;rsquo;d like, and our conversations might not indicate that we truly care about what is going on in a friend&amp;rsquo;s life if we&amp;rsquo;ve forgotten what major life-experiences they&amp;rsquo;re going through. These mishaps might not be overtly bad, but they can prevent us from having truly connected relationships. Who wouldn&amp;rsquo;t want better relationships?&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s better, is that Monica open-sources its code and emphasizes privacy. When it&amp;rsquo;s suggested that you aren&amp;rsquo;t being tracked and that your data isn&amp;rsquo;t being sold — a significant promise in an age where everybody is out for your data — it is verifiable.&lt;/p&gt;

&lt;p&gt;I was initially pulled toward the platform, but two pieces of anecdata really spoke to me and cemented my feelings enough to try it out:&lt;/p&gt;

&lt;p&gt;1) &lt;em&gt;danielvf&lt;/em&gt; from Hacker News &lt;a href=&#34;https://news.ycombinator.com/item?id=14498590&#34;&gt;writes&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My uncle died suddenly this year. He was unbelievably caring - and not just to family -
but to everyone he ever met. His funeral was jam packed with everyone
from homeless people to executives of multi-billion dollar companies.&lt;/p&gt;

&lt;p&gt;I always thought that his ability to always have you, and whatever you had
last talked about with him, on his mind at any moment was some kind of
supernatural gift. I was surprised to find out at his funeral that he
actually kept an excel spreadsheet of everyone he met and what they
needed and were going through. He reviewed this constantly.&lt;/p&gt;

&lt;p&gt;It didn&amp;rsquo;t lessen his genuine love for everyone, just let him be a little more super human.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;2) In the book &lt;strong&gt;&lt;a href=&#34;https://www.amazon.com/How-Win-Friends-Influence-People/dp/0671027034&#34;&gt;How to Win Friends and Influence People&lt;/a&gt;&lt;/strong&gt;, a tip for getting the most out of the book is suggested:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The president of an important Wall Street bank once described, in
a talk before one of my classes, a highly efficient system he used for
self-improvement. This man had little formal schooling; yet he had
become one of the most important financiers in America, and he
confessed that he owed most of his success to the constant
application of his homemade system. This is what he does, I&amp;rsquo;ll put it
in his own words as accurately as I can remember.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;For years I have kept an engagement book showing all the
appointments I had during the day. My family never made any plans
for me on Saturday night, for the family knew that I devoted a part
of each Saturday evening to the illuminating process of self-examination
and review and appraisal. After dinner I went off by
myself, opened my engagement book, and thought over all the
interviews, discussions and meetings that had taken place during the
week. I asked myself:&lt;/p&gt;

&lt;p&gt;&amp;lsquo;What mistakes did I make that time?&amp;rsquo; &amp;lsquo;What did I do that was right and
in what way could I have improved my performance?&amp;rsquo; &amp;lsquo;What
lessons can I learn from that experience?&amp;rsquo;
&amp;ldquo;I often found that this weekly review made me very unhappy. I was
frequently astonished at my own blunders. Of course, as the years
passed, these blunders became less frequent. Sometimes I was
inclined to pat myself on the back a little after one of these sessions.
This system of self-analysis, self-education, continued year after
year, did more for me than any other one thing I have ever
attempted.
&amp;ldquo;It helped me improve my ability to make decisions - and it aided me
enormously in all my contacts with people. I cannot recommend it
too highly.&amp;rdquo;
Why not use a similar system to check up on your application of the
principles discussed in this book? If you do, two things will result.
First, you will find yourself engaged in an educational process that is
both intriguing and priceless.
Second, you will find that your ability to meet and deal with people
will grow enormously.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I think I could use Monica as a tool for holding myself accountable when engaging with others, in a consistent, meaningful way. Monica could also facilitate the sort of introspection noted above, with its journal, activity, and reminder features. But while this is hugely appealing, I have some concerns. What if there was a breach?&lt;/p&gt;

&lt;h1 id=&#34;monica-in-the-crosshairs&#34;&gt;Monica in the Crosshairs&lt;/h1&gt;

&lt;p&gt;A breach would be very dangerous. Arguably Monica stores more information (not data) than Facebook. Where a picture on Facebook might represent an event, its participants, and its location, this is described in plaintext on Monica, making it easier to draw insights. A computer can parse this text much more easily than images. Additionally, since Monica is a privacy-oriented social manager, there is more of a &lt;em&gt;feeling&lt;/em&gt; of personal security, which could likely result in far more personal disclosure, in other words the willful release of more intimate details. In some sense, this makes users more vulnerable, personally, if there ever is such a breach. For those that want to draw parallels to encrypted messaging apps, I think the average person is going to write the same content in an SMS as they will a Signal message, and even if not, as of right now these encrypted-messaging apps are far more secure, so the targeting is less of a concern. I will, however, draw parallels to Facebook, because even though Monica isn&amp;rsquo;t a social network, it is in many ways comparable, and can serve many of Facebook&amp;rsquo;s primary functions.&lt;/p&gt;

&lt;p&gt;Topics of private conversation by definition aren&amp;rsquo;t shared on Facebook, but could be on Monica. Details of a relative or friend in a hospital could be implicitly and explicitly exploited. Drug addiction? Financial or spousal trouble? These are all negatives that could be taken advantage of, or could form the basis of a spear-phishing campaign. Even general data like a user&amp;rsquo;s workplace is often spoofed on Facebook, but on a platform like Monica, people will probably be more inclined to submit honest data.&lt;/p&gt;

&lt;p&gt;Many people first joined Facebook because they wanted to keep track of friends and family in the way Monica does. Facebook largely shifted, became less trustworthy, and became more superficial in users&amp;rsquo; eyes. A social manager that is open-sourced and private (where people are actually themselves and don&amp;rsquo;t self-censor), is an awesome alternative to something like Facebook. But it is still worth exploring these considerations before committing large amounts of personal information to a web platform that may be extra prone to being targeted. Is there 2FA? Not yet. What security measures do their staff take when dealing with lockouts or other requests? I&amp;rsquo;m sure more is to come as the platform scales. I would still personally choose Monica over other platforms any day. I just think users should be at least a little conscious of what kind of data they&amp;rsquo;re adding to their page, without assuming it will be perfectly secure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Projecting with Python [GIS, Python]</title>
      <link>https://ckhoward.github.io/blog/projecting-with-python-gis-python/</link>
      <pubDate>Fri, 09 Jun 2017 12:28:59 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/blog/projecting-with-python-gis-python/</guid>
      <description>

&lt;h1 id=&#34;my-introduction-to-gis-with-python&#34;&gt;My Introduction to GIS with Python&lt;/h1&gt;

&lt;p&gt;Python is a powerful tool in the GIS world, so I wanted to get a little bit of practice with it. I have had a lot of fun working with the &lt;a href=&#34;http://www.start.umd.edu/gtd/&#34;&gt;Global Terrorism Database&lt;/a&gt; so I figured I would go from its CSV format to one that is better-supported by GIS — the shapefile. The dataset contains information related to terrorist attacks, including attack locations. Each location has a variety of data but I will focus on country, latitude, and longitude. Specifically, I will observe attacks in Iraq. The coordinates are based on WGS1984 standards, so they will have to be converted to UTM Zone 38N in order to be mapped on a flat-surface (more on this later, no worries if you don&amp;rsquo;t understand).&lt;/p&gt;

&lt;h1 id=&#34;first-step-cleaning-the-data&#34;&gt;First Step: Cleaning the Data&lt;/h1&gt;

&lt;p&gt;When we first get the data, it contains way more information than we need. There are over 120 variables and as noted, we primarily want to focus on locational data and a few other variables like target type, attack type, and terrorist group responsible. We need to get rid of excess so that our script runs faster and is more relevant to our objective.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/dataset.jpg&#34; alt=&#34;alt text&#34; title=&#34;Global Terrorism Database&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can see the list of imported modules. Pandas is used first to load the data into a DataFrame (basically a table) to be operated on. Functional programming is best suited for the overall task, as for most data analysis, so here is a simple function to clean the data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection1.jpg&#34; alt=&#34;alt text&#34; title=&#34;Cleaning the Data with Pandas Read&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the event that the &lt;code&gt;clean_csv()&lt;/code&gt; hasn&amp;rsquo;t been called yet, call it. If it has already been called, don&amp;rsquo;t worry about it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection2.jpg&#34; alt=&#34;alt text&#34; title=&#34;Cleaning the Data with Pandas Read&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now we&amp;rsquo;ve cleaned our data and created a DataFrame with most of what we want. Now we can use another function to specify which country we&amp;rsquo;re interested in.&lt;/p&gt;

&lt;h1 id=&#34;second-step-specifying-the-country-of-interest&#34;&gt;Second Step: Specifying the Country of Interest&lt;/h1&gt;

&lt;p&gt;In our case, we&amp;rsquo;re interested in looking at Iraq. We want to see all of the terrorist attacks that occurred in Iraq. We will clean the frame a little bit more now. This step could have been done in the first function but I believe it&amp;rsquo;s more simple for a future user to read, understand, and plug the country of interest with a separate function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection3.jpg&#34; alt=&#34;alt text&#34; title=&#34;Specifying the Country of Interest - Iraq&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Simple enough. The DataFrame object from the last function is loaded in and we create a new frame that meets our criteria; where every record&amp;rsquo;s country value is &amp;ldquo;Iraq.&amp;rdquo; Then, any records containing NaNs are dropped. Our coordinate data is still based on the WGS1984 standard — they work on 3D planes, like in Google Earth, but not on 2D planes, like almost every other map in existence. This calls for conversion.&lt;/p&gt;

&lt;h1 id=&#34;third-step-converting-the-coordinates&#34;&gt;Third Step: Converting the Coordinates&lt;/h1&gt;

&lt;p&gt;Coordinate systems use values that assume 3D-space and are provided by satellites. The problem is, most maps are flat, and when you&amp;rsquo;re flattening the earth there are always distortions. These distortions affect distance, direction, and scale. Check out Kai Chang&amp;rsquo;s &lt;a href=&#34;https://bl.ocks.org/syntagmatic/ba569633d51ebec6ec6e&#34;&gt;awesome visualizations&lt;/a&gt; for comparing different map projections.&lt;/p&gt;

&lt;p&gt;Mathematical formulae are used to convert geographic coordinates into their flat-plane equivalents. The Python module &lt;a href=&#34;https://github.com/jswhit/pyproj&#34;&gt;Pyproj&lt;/a&gt; handles this for users, so I import this module and use it in my conversion function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection4.jpg&#34; alt=&#34;alt text&#34; title=&#34;Python Pyproj to Convert from Geographic Coordinates to Projected UTM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You may have noticed I used the UTM projection (Universal Transverse Mercator). This is because distortion occurs at the poles of the earth, whereas the areas around the equator tend to be accurately preserved. This is good for us since we&amp;rsquo;re observing Iraq. Iraq falls in the Zone 38N band, and must be used as an argument.&lt;/p&gt;

&lt;h3 id=&#34;incorrect-pyproj-value-output&#34;&gt;Incorrect Pyproj Value Output&lt;/h3&gt;

&lt;p&gt;Here, I made a common mistake. I was getting pretty wildly wrong values, where points were showing up in Ethiopia and not Iraq. Note the values for utm_lat and utm_lon toward the bottom:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection_output1.jpg&#34; alt=&#34;alt text&#34; title=&#34;Incorrect Pyproj Output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I wanted to verify whether or not these values were right, because I knew upon plotting them, I was getting Ethiopian areas, so I used the &lt;a href=&#34;http://home.hiwaay.net/~taylorc/toolbox/geography/geoutm.html&#34;&gt;Geographic/UTM Coordinate Converter Website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/conversion_site.jpg&#34; alt=&#34;alt text&#34; title=&#34;Coordinate to UTM Web Tool&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Upon looking at the UTM outputs, this is obviously wrong:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;-139638 != 354377&lt;/li&gt;
&lt;li&gt;483126 != 410805&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My problem was that I had the parameters switched. In simple terms, I had p2(latitude, longitude), but I should have had p2(longitude, latitude). It turns out, in many Python GIS modules, longitude comes before latitude. When I changed the order of these parameters, the output was correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection_output2.jpg&#34; alt=&#34;alt text&#34; title=&#34;Correct Pyproj Output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now that all of the outputs are correct, we can go to the final step.&lt;/p&gt;

&lt;h1 id=&#34;fourth-and-final-step-producing-a-shapefile-with-our-new-data&#34;&gt;Fourth and Final Step: Producing a Shapefile with our New Data&lt;/h1&gt;

&lt;p&gt;To create a shapefile I decided I would use Python&amp;rsquo;s Fiona and Shapely modules.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection5.jpg&#34; alt=&#34;alt text&#34; title=&#34;Using Fiona to Create a Shapefile&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The previous two functions are called:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection6.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Convert and Write Functions are Called&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now the shapefile has been created:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection7.jpg&#34; alt=&#34;alt text&#34; title=&#34;Shapefiles in Folder&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;using-arcgis-to-map-the-shapefile&#34;&gt;Using ArcGIS to Map the Shapefile&lt;/h1&gt;

&lt;p&gt;Open ArcMap. Go to the Catalog tab and click it. Navigate to the folder. Find the shapefile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection8.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Shapefile in ArcGIS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Right click the shapefile. Go to properties. In the &amp;lsquo;XY Coordinate System&amp;rsquo; tab, expand the &amp;lsquo;Projected Coordinate Systems&amp;rsquo; folder, expand UTM, expand WGS 1984, expand Northern Hemisphere, and then find &amp;lsquo;WGS 1984 UTM Zone 38N.&amp;rsquo; Click it and press the &amp;lsquo;Apply&amp;rsquo; and &amp;lsquo;OK&amp;rsquo; buttons. Drag the shapefile onto your canvas. And don&amp;rsquo;t forget to go to &amp;lsquo;Add Data&amp;rsquo; and &amp;lsquo;Add Basemap&amp;rsquo; to find a map to sit under the points — I chose the dark gray basemap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection10.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Shapefile Mapped&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As the code indicated, I was also recording information on the target types of the attacks. When I use the identify tool on a point (or attack location), or when I open the attribute table, this information will be associated with the geometry (in our case, the point).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection11.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Shapefile&#39;s Associated Properties&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;These functions are simple but they have vast implications. All I did was take a CSV with spatial data, filtered out data that I wasn&amp;rsquo;t interested in, transformed the data into something that I could actually work with, and then wrote the data to a shapefile. These processes give me the ability to do further geospatial analysis and to build and deploy an interactive and informative website regarding terrorist attacks. In the future, I definitely intend to take advantage of other Python geospatial libraries for map-making, like Descartes and Basemap.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Projecting with Python [GIS, Python]</title>
      <link>https://ckhoward.github.io/post/Projecting-with-Python-%5BGIS,-Python%5D/</link>
      <pubDate>Fri, 09 Jun 2017 12:28:59 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/post/Projecting-with-Python-%5BGIS,-Python%5D/</guid>
      <description>

&lt;h1 id=&#34;my-introduction-to-gis-with-python&#34;&gt;My Introduction to GIS with Python&lt;/h1&gt;

&lt;p&gt;Python is a powerful tool in the GIS world, so I wanted to get a little bit of practice with it. I have had a lot of fun working with the &lt;a href=&#34;http://www.start.umd.edu/gtd/&#34;&gt;Global Terrorism Database&lt;/a&gt; so I figured I would go from its CSV format to one that is better-supported by GIS — the shapefile. The dataset contains information related to terrorist attacks, including attack locations. Each location has a variety of data but I will focus on country, latitude, and longitude. Specifically, I will observe attacks in Iraq. The coordinates are based on WGS1984 standards, so they will have to be converted to UTM Zone 38N in order to be mapped on a flat-surface (more on this later, no worries if you don&amp;rsquo;t understand).&lt;/p&gt;

&lt;h1 id=&#34;first-step-cleaning-the-data&#34;&gt;First Step: Cleaning the Data&lt;/h1&gt;

&lt;p&gt;When we first get the data, it contains way more information than we need. There are over 120 variables and as noted, we primarily want to focus on locational data and a few other variables like target type, attack type, and terrorist group responsible. We need to get rid of excess so that our script runs faster and is more relevant to our objective.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/dataset.jpg&#34; alt=&#34;alt text&#34; title=&#34;Global Terrorism Database&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can see the list of imported modules. Pandas is used first to load the data into a DataFrame (basically a table) to be operated on. Functional programming is best suited for the overall task, as for most data analysis, so here is a simple function to clean the data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection1.jpg&#34; alt=&#34;alt text&#34; title=&#34;Cleaning the Data with Pandas Read&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In the event that the &lt;code&gt;clean_csv()&lt;/code&gt; hasn&amp;rsquo;t been called yet, call it. If it has already been called, don&amp;rsquo;t worry about it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection2.jpg&#34; alt=&#34;alt text&#34; title=&#34;Cleaning the Data with Pandas Read&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now we&amp;rsquo;ve cleaned our data and created a DataFrame with most of what we want. Now we can use another function to specify which country we&amp;rsquo;re interested in.&lt;/p&gt;

&lt;h1 id=&#34;second-step-specifying-the-country-of-interest&#34;&gt;Second Step: Specifying the Country of Interest&lt;/h1&gt;

&lt;p&gt;In our case, we&amp;rsquo;re interested in looking at Iraq. We want to see all of the terrorist attacks that occurred in Iraq. We will clean the frame a little bit more now. This step could have been done in the first function but I believe it&amp;rsquo;s more simple for a future user to read, understand, and plug the country of interest with a separate function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection3.jpg&#34; alt=&#34;alt text&#34; title=&#34;Specifying the Country of Interest - Iraq&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Simple enough. The DataFrame object from the last function is loaded in and we create a new frame that meets our criteria; where every record&amp;rsquo;s country value is &amp;ldquo;Iraq.&amp;rdquo; Then, any records containing NaNs are dropped. Our coordinate data is still based on the WGS1984 standard — they work on 3D planes, like in Google Earth, but not on 2D planes, like almost every other map in existence. This calls for conversion.&lt;/p&gt;

&lt;h1 id=&#34;third-step-converting-the-coordinates&#34;&gt;Third Step: Converting the Coordinates&lt;/h1&gt;

&lt;p&gt;Coordinate systems use values that assume 3D-space and are provided by satellites. The problem is, most maps are flat, and when you&amp;rsquo;re flattening the earth there are always distortions. These distortions affect distance, direction, and scale. Check out Kai Chang&amp;rsquo;s &lt;a href=&#34;https://bl.ocks.org/syntagmatic/ba569633d51ebec6ec6e&#34;&gt;awesome visualizations&lt;/a&gt; for comparing different map projections.&lt;/p&gt;

&lt;p&gt;Mathematical formulae are used to convert geographic coordinates into their flat-plane equivalents. The Python module &lt;a href=&#34;https://github.com/jswhit/pyproj&#34;&gt;Pyproj&lt;/a&gt; handles this for users, so I import this module and use it in my conversion function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection4.jpg&#34; alt=&#34;alt text&#34; title=&#34;Python Pyproj to Convert from Geographic Coordinates to Projected UTM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You may have noticed I used the UTM projection (Universal Transverse Mercator). This is because distortion occurs at the poles of the earth, whereas the areas around the equator tend to be accurately preserved (due to the sizes of the zones). This is good for us since we&amp;rsquo;re observing Iraq. Iraq falls in the Zone 38N band, and must be used as an argument.&lt;/p&gt;

&lt;h3 id=&#34;incorrect-pyproj-value-output&#34;&gt;Incorrect Pyproj Value Output&lt;/h3&gt;

&lt;p&gt;Here, I made a common mistake. I was getting pretty wildly wrong values, where points were showing up in Ethiopia and not Iraq. Note the values for utm_lat and utm_lon toward the bottom:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection_output1.jpg&#34; alt=&#34;alt text&#34; title=&#34;Incorrect Pyproj Output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I wanted to verify whether or not these values were right, because I knew upon plotting them, I was getting Ethiopian areas, so I used the &lt;a href=&#34;http://home.hiwaay.net/~taylorc/toolbox/geography/geoutm.html&#34;&gt;Geographic/UTM Coordinate Converter Website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/conversion_site.jpg&#34; alt=&#34;alt text&#34; title=&#34;Coordinate to UTM Web Tool&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Upon looking at the UTM outputs, this is obviously wrong:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;-139638 != 354377&lt;/li&gt;
&lt;li&gt;483126 != 410805&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My problem was that I had the parameters switched. In simple terms, I had p2(latitude, longitude), but I should have had p2(longitude, latitude). It turns out, in many Python GIS modules, longitude comes before latitude. When I changed the order of these parameters, the output was correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection_output2.jpg&#34; alt=&#34;alt text&#34; title=&#34;Correct Pyproj Output&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now that all of the outputs are correct, we can go to the final step.&lt;/p&gt;

&lt;h1 id=&#34;fourth-and-final-step-producing-a-shapefile-with-our-new-data&#34;&gt;Fourth and Final Step: Producing a Shapefile with our New Data&lt;/h1&gt;

&lt;p&gt;To create a shapefile I decided I would use Python&amp;rsquo;s Fiona and Shapely modules.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection5.jpg&#34; alt=&#34;alt text&#34; title=&#34;Using Fiona to Create a Shapefile&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The previous two functions are called:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection6.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Convert and Write Functions are Called&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now the shapefile has been created:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection7.jpg&#34; alt=&#34;alt text&#34; title=&#34;Shapefiles in Folder&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;using-arcgis-to-map-the-shapefile&#34;&gt;Using ArcGIS to Map the Shapefile&lt;/h1&gt;

&lt;p&gt;Open ArcMap. Go to the Catalog tab and click it. Navigate to the folder. Find the shapefile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection8.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Shapefile in ArcGIS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Right click the shapefile. Go to properties. In the &amp;lsquo;XY Coordinate System&amp;rsquo; tab, expand the &amp;lsquo;Projected Coordinate Systems&amp;rsquo; folder, expand UTM, expand WGS 1984, expand Northern Hemisphere, and then find &amp;lsquo;WGS 1984 UTM Zone 38N.&amp;rsquo; Click it and press the &amp;lsquo;Apply&amp;rsquo; and &amp;lsquo;OK&amp;rsquo; buttons. Drag the shapefile onto your canvas. And don&amp;rsquo;t forget to go to &amp;lsquo;Add Data&amp;rsquo; and &amp;lsquo;Add Basemap&amp;rsquo; to find a map to sit under the points — I chose the dark gray basemap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection10.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Shapefile Mapped&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As the code indicated, I was also recording information on the target types of the attacks. When I use the identify tool on a point (or attack location), or when I open the attribute table, this information will be associated with the geometry (in our case, the point).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/projection11.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Shapefile&#39;s Associated Properties&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;These functions are simple but they have vast implications. All I did was take a CSV with spatial data, filtered out data that I wasn&amp;rsquo;t interested in, transformed the data into something that I could actually work with, and then wrote the data to a shapefile. These processes give me the ability to do further geospatial analysis and to build and deploy an interactive and informative website regarding terrorist attacks. In the future, I definitely intend to take advantage of other Python geospatial libraries for map-making, like Descartes and Basemap.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>path distance analysis</title>
      <link>https://ckhoward.github.io/post/path-distance-analysis/</link>
      <pubDate>Tue, 30 May 2017 19:24:32 -0700</pubDate>
      
      <guid>https://ckhoward.github.io/post/path-distance-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visibility Analysis [GIS]</title>
      <link>https://ckhoward.github.io/blog/visibility-analysis-gis/</link>
      <pubDate>Fri, 26 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ckhoward.github.io/blog/visibility-analysis-gis/</guid>
      <description>

&lt;p&gt;&lt;em&gt;This post is intended to be informative, and not so much reproducible.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;viewshed-analysis&#34;&gt;Viewshed Analysis&lt;/h2&gt;

&lt;p&gt;A viewshed is an area that is visible from an observer at a given location. The problem of visibility lends itself to many domains:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a guard on a prison tower must see the entire prison yard to ensure everyone&amp;rsquo;s safety&lt;/li&gt;
&lt;li&gt;a scout must maximize visibility of a battlefield for intelligence gathering&lt;/li&gt;
&lt;li&gt;autonomous vehicles must have 360-degree visibility&lt;/li&gt;
&lt;li&gt;cell-towers must provide maximum coverage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, we will pay mind to the ancient world, where according to Christopherson and Guertin*, &amp;ldquo;fortified sites were often located in order to visually control their territory, sacred sites might be located to provide views of other sacred sites, and the settlement patterns of hinterland sites might be located to facilitate, or to impede, visual communication.&amp;rdquo; In particular, we will be observing and analyzing the Jordanian Umayri Wall&amp;rsquo;s Viewshed. If somebody (that is, say, 6&amp;rsquo; tall), were standing on top of this wall, what would she see?&lt;/p&gt;

&lt;h2 id=&#34;umayri-wall-viewshed&#34;&gt;Umayri Wall Viewshed&lt;/h2&gt;

&lt;p&gt;This is the Viewshed of the Tall Umayri archaeological site. The red star indicates the site, while the white space indicates area that is not visible from the site, and the blue space indicates area that is visible from the site.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri.jpg&#34; alt=&#34;alt text&#34; title=&#34;A Woman&#39;s Visibility when Standing on Wall&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If we want to specifically see the wall, we can zoom in a little bit; the wall is indicated by the red ring. Visualizing what is visible from a given point is a powerful ability. If you noticed, this image is quite pixelated; Viewshed analyses operate on digital elevation models, which are rasters. These analyses will only work on raster data, and not vector data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri2.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Umayri Wall, Indicated by the Red Ring&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;raster-calculation&#34;&gt;Raster Calculation&lt;/h2&gt;

&lt;p&gt;Viewsheds are rasters. GIS allows us to perform raster calculations. In ArcGIS, we can open the raster calculator and use a conditional statement (as shown below) to create a new binary raster; 0 if the cell is not &amp;lsquo;visible&amp;rsquo; and falls within the boundary, and 1 if the cell is visible and within the boundary.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri4.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Conditional Statement, if visible, 1, else 0&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Because we have the total number of cells (not shown), and we have the number of cells that belong to both 0 (non-visibility) and 1 (visibility), we can calculate the percentages of both by non-visibility/total and visibility/total. As shown, about 17% of the cells are visible, while the remaining 83% of cells are not visible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri3.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Umayri Wall, Indicated by the Red Ring&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;more-raster-calculation&#34;&gt;More Raster Calculation&lt;/h2&gt;

&lt;p&gt;When considering viewsheds, a useful raster calculation is combining the viewshed of different points. Returned data is visualized in such a way that you can see what areas are visible from both locations, or either location. So that is what we will do, using site2 (the green star). In this example, the magenta areas can be seen from the green star, the blue from the red star, the purple from both stars, and the white from neither.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri5.jpg&#34; alt=&#34;alt text&#34; title=&#34;Combined Visibility from the Green and Red Stars&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-impact-of-combined-viewsheds&#34;&gt;The Impact of Combined Viewsheds&lt;/h2&gt;

&lt;p&gt;Tying back into the original application domains, to get full view of a prison yard, towers can be added (or raised). To provide full coverage to a city, cellular towers can be added. And to gain &amp;lsquo;full control&amp;rsquo; over ancient sites, they too could put up more towers. So we will pretend they did, and we will see how much more visibility these added towers gave them. Our added towers will be indicated with white stars. Their respective viewsheds are indicated by the varying colors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri6.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Viewshed From Umayri with 6 Added Towers&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To get percentages of area covered, we can use the conditional statement from earlier. Remember that non-visible areas will be represented as 0&amp;rsquo;s and visible areas will be represented as 1&amp;rsquo;s. With some simple division, we can see that the new viewshed is far better, with 53% of the area being visible (as opposed to 17%), and 47% of the area not being visible (as opposed to 83%).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;img/umayri7.jpg&#34; alt=&#34;alt text&#34; title=&#34;The Viewshed from Umayri with 6 Added Towers&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Though this is a significant improvement, it is far from ideal. The towers were added at arbitrarily chosen sites (green points). This step in the analysis could have been optimized for the best visibility, with more emphasis on western sites than eastern, or more emphasis on high-elevation points. This simple analysis can be of tremendous value to any problem concerning visibility.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.casa.arizona.edu/MPP/viewshed/vspaper.html&#34;&gt;* Visibility Analysis and Ancient Settlement Strategies&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>